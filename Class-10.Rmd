---
title: ""
---

<!--

library(knitr)
rmarkdown::render_site("Class-10.Rmd")# build website

# rmarkdown::render_site()# build webpage
# COPY FILES TO DOCS FOR GITHUB.IO
system(paste("xcopy", 
    '"C:/Users/mcolvin/Documents/Teaching/WFA8433-Natural-Resource-Decision-Making/Course-Materials/_site"', 
    '"C:/Users/mcolvin/Documents/Teaching/WFA8433-Natural-Resource-Decision-Making/Docs"',
    "/E /C /H /R /K /O /Y")) 
  q(save="no") 
-->



```{r echo=FALSE, out.width="95%"}
include_graphics("media/banner-09.jpg")
rm(list=objects())
```
<!--
Homework 1:  Introduction to basic computing- R
List of preliminary problems to instructor for review
-->

# Class 10. LMs and GLMs in Decision Contexts

* Supplemental background reading for next class(es): 
* Assignment due: Objectives network part II (Wednesday by 12 pm). See 
details [here](hw-01.html) 
* Group work: Continue discussing potential class projects
* Link to class recording  [YouTube](https://youtu.be/Wd4iGZcOuk8)
* Today's R script [Class-10.R](scripts/Class-10.R)

## Objectives 

By the end of this tutorial you should be able to:

1. Understand linear models 
1. Understand log transformation
1. Understand interactions in linear models
2. The effect of varying distributions 


<!--
3. Using model selection for inference
-->

<center>  
```{r echo=FALSE, out.width="35%"}
include_graphics("media/best-bears.jpg")
```  
</center>


## Some preliminaries

* If you want to play along in class download this zipfile 
[*.zip](http://mec685.cfr.msstate.edu/class-10.zip). 
* Be sure to unzip it before trying to use files and such
* The file contains the dataset used in class and an R script of all the 
code. 
* Once you have it where you want open the R script and be sure to check the working directory `getwd()` and make
sure it is where your folder is.  
* If your working directory is not correct, you can set it in Rstudio: 
"Session --> Set Working Directory --> To source file location". Or you 
can use the `setwd()` in the console. 


## Background and the data

Despite current debates about which bear is best 
[1](https://youtu.be/N8d86Kjl1dg), we will be modeling black bear home 
ranges using a linear model. Linear and generalized linear models are 
essentially the same thing. There is a response variable, home range in 
this case. Variability in home range is then explained using a linear 
combination of covariates. Recall, traditionally is can be expressed as: 



$$Y_i = \beta_{0} + \beta_{1}\cdot X_{i} + \epsilon_i$$

where 

* Y is home range in square kilometers, 
* $\beta_{0}$ is the model intercept, 
* $\beta_{1}$ is the effect of covariate $X$ on home range size, 
* $i$ indexes individual bear, and 
* $\epsilon$ is the error. 

The parameter $\epsilon$ is a normally distributed value with mean 0 and 
some level of variability represented as a standard deviation. 
Covariates in a linear model can be categorical (e.g., sex, stage) or 
continuous (e.g., weight). The model can also include an interaction of 
covariates. 






Preliminaries done, let's look at the data. But first we need to read it 
in. 




```{r}
dat<-read.csv("homerange-data.csv")
```

Let's make sure things look good.

```{r}
head(dat)
```

and check the types of data that are read in

```{r}
str(dat)
```

Everything looks good. The sex variable is a factor, as it should be. 
The weight and the home range variable is a numeric. 



```{r}
boxplot(homerange~sex,data=dat)

hist(dat$homerange)
```

Ugggh. That does not look normal to me, but let's go with it and fit a 
linear model and see what happens. 



## A basic linear model of a normally distributed variable


Let's fit a linear model to some the data. Suppose we are interested in 
predicting whether female bears have larger or smaller home ranges than 
male bears and the associated uncertainty. The model for this can be 
recast as: 


$$\mu = \beta_0 + \beta_1\cdot \text{Sex}+
    \beta_2\cdot\text{Weight}+
    \beta_3\cdot\text{Weight}\cdot\text{Sex} $$

where,

* $\beta_0$ is the intercept of the model
* $\beta_1$ is the effect of sex.
* $\beta_2$ is the effect of weight.
* $\beta_3$ is the interaction of sex and weight.

We can link the predictive model to the data using a statistical model. 
In this case we are assuming a normal distribution with some uncertainty 
quantified as $\sigma$. Recall was specific this as: 



$$Y_i \sim Normal(\mu,\sigma)$$

where

* $\mu$ is the prediction
* $\sigma$ is the standard deviation.



Now, I crafted this comparison deliberately to refine some 
understanding of R and linear models. First we need to examine the 
default way R treats categorical variables, like sex. We can figure out 
which factor is 'first' using the `levels()` function. 


```{r}
levels(dat$sex)
```

Female is listed first then male. By default, R orders levels of 
categorical variables alphabetically. Let's see how it is treated in a 
linear model. 


```{r}
fit<-lm(homerange~sex,data=dat) # fit the model
summary(fit) # model summary
coef(fit) # extract estimated coefficients
betas<-coef(fit) # save betas
```

The estimate of the intercept is `r round(coef(fit)[1],3)` and the 
effect of sex is `r round(coef(fit)[2],3)`. Notice the name of the 
effect of sex is called `sexmale`. This is the effect of being male on 
home range size. The intercept represents the expected home range of a 
female bear. The expected home range of a male bear is then `r 
round(coef(fit)[1],3)`+ `r round(coef(fit)[2],3)` = `r 
round(coef(fit)[1],3) + round(coef(fit)[2],3)` km, given this model. 
Recall we were interested in whether female bears had a larger or 
smaller home range. While you can evaluate with the model we fit, 
switching the order so male is the first level and would therefore be 
the intercept, would make sense. The second estimated coefficient is 
then the effect of being female. To do this we need to reorder the 
factor levels in R which can be done with the `factor()` function. 


```{r}
dat$sex<- factor(dat$sex,levels=c("male","female")) # reorder factor levels
``` 
 
Now let's refit the same model and see what happens now.

```{r}
fit<- lm(homerange~sex,data = dat)
summary(fit) # model summary
coef(fit) # estimated coefficients
```

Now the expected homerange for a male bear is the intercept and a 
female bear is `r round(coef(fit)[1],3)`+ `r round(coef(fit)[2],3)` = 
`r round(coef(fit)[1],3) + round(coef(fit)[2],3)` km, given this model. 
Compare the expected home range for a female bear with the intercept for 
the first model fit, the are the same. Formally, the model estimates 3 
parameters: 



* $\beta_{0}$ = `r round(coef(fit)[1],3)` this is the average home range of a male bear
* $\beta_{1}$ = `r round(coef(fit)[2],3)` the effect of being a female on home range
* $\sigma$ = `r round(summary(fit)$sigma,3)` residual standard error

Ok, now we also suspect there is an effect of weight on home range, 
larger bears have large home ranges. The effect of weight can be 
evaluated by adding weight to the linear model. The model parameters for 
this model are the same as before but there is a new parameter 
estimated, $\beta_{2}$ which is the effect of weight on home range. 
Let's fit the model. 



```{r}
fit<-lm(homerange~sex+weight+sex:weight,data=dat)
summary(fit)
coef(fit)
```

Well I'll be the model fits pretty well. The r-squared value is `r 
summary(fit)$r.squared` which is pretty good for ecological data, right? 
Either way the parameter estimates for the model are: 


* $\beta_{0}$ = `r round(coef(fit)[1],3)` this is the average home range of a male bear
* $\beta_{1}$ = `r round(coef(fit)[2],3)` the effect of being a female on home range
* $\beta_{2}$ = `r round(coef(fit)[3],3)` the effect of weight on home range
* $\beta_{3}$ = `r round(coef(fit)[4],3)` the effect of weight on home range
* $\sigma$ = `r round(summary(fit)$sigma,3)` residual standard error


## Making predictions from the model

Neat, we have fit a model that estimates home range given bear sex and 
weight. We can use the model to predict home ranges for male and female 
bears of varying weights. Let's do that and see how good our model 
works, the anticipation is killing me! First we need a dataset of male 
and female bears to predict for. Bear weights vary from `r 
round(min(dat$weight),1)` to `r round(max(dat$weight),1)` and therefor 
we should predict between those values. Above or below those values you 
are getting into the realm of extrapolation. 

```{r}
newdat<- data.frame(weight=seq(min(dat$weight), max(dat$weight),1))
head(newdat)
```

But wait, don't we need sex in the `data.frame` too? Right you are, 
let's add it. We can do this by 

```{r}
newdat$sex<-"male"
preddat<- newdat
newdat$sex<-"female"
preddat<- rbind(preddat,newdat)
```

Let's double check the data to make sure it is formatted correctly.

```{r}
str(preddat)
```

Hmmmm, that is not quite right. Sex is a character, it should be a 
factor. Let's fix that. 

```{r}
preddat$sex<- factor(preddat$sex,levels=c("male","female"))
levels(preddat$sex)
```

Ok, that looks better, especially the ordering of the levels. Now back 
to the fun stuff, let's get predicting with the `predict()` function. 

```{r}
preddat$pred<- predict(fit,preddat)
head(preddat)
```

Now we can plot the predictions.

```{r}
plot(pred~weight,preddat,type='n',xlab="Weight",ylab="Home range (km)")
points(pred~weight,preddat,subset=sex=="male",type='l',lty=1)
points(pred~weight,preddat,subset=sex=="female",type='l',lty=2)
legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

Ok, I am fisheries biologist not a rocket surgeon, but that does not 
seem right, how can home ranges be negative? Let's plot it with the 
original data. 


```{r}
plot(homerange~weight,dat,type='n',xlab="Weight",ylab="Home range (km)",ylim=c(-1,10))
points(homerange~weight,dat,subset=sex=="male",pch=1)
points(homerange~weight,dat,subset=sex=="female",pch=2)

points(pred~weight,preddat,subset=sex=="male",type='l',lty=1)
points(pred~weight,preddat,subset=sex=="female",type='l',lty=2)
legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

That is a terrible fit. But, the r-squared value was `r 
summary(fit)$r.squared`? I probably should have looked at the data 
first, but that is part of the point of this example. There are a couple 
of things that might be going on here. First, the distribution could be 
wrong. That data does not look all the normally distributed. Second, the 
model could be wrong and home ranges are not linearly related to sex and 
weight. We can evaluate these 2 problems with plots of residuals versus 
predicted to evaluate distribution and observed versus predicted values 
to evaluate if the model is misspecified. Here we go. 



```{r}
dat$resids<-resid(fit)
dat$preds<-fitted(fit)

# plot of residuals versus predicted
# should be normal and centered around 0
plot(resids~preds,dat)
abline(h=0)
```

Well that does not good at all. The residuals are not centered around 0, 
the model tends to over predict at low values, under predict at 
intermediate values, and over predict at high values. 


 

```{r}
# plot of observed versus predicted
# should fall on a 1:1 line
plot(homerange~preds,dat)
abline(0,1)
```

That does not look good at all either. The model appears to be 
misspecified and some unaccounted for curvature in the model. Let's 
tackle the distribution issue to see if that fixes things. 



## Messing with the distribution

Some cool kids told me that a Poisson might be the way to go because it 
models things 0 or greater. Home ranges are greater than 0 and it makes 
good sense to not let home range dip below 0. A linear model with a 
Poisson distribution can be fit using the `glm()` function. 

recast as: 


$$log(\mu) = \beta_0 + \beta_1\cdot \text{Sex}+
    \beta_2\cdot\text{Weight}+
    \beta_3\cdot\text{Weight}\cdot\text{Sex} $$

where,

* $\beta_0$ is the intercept of the model
* $\beta_1$ is the effect of sex.
* $\beta_2$ is the effect of weight.
* $\beta_3$ is the interaction of sex and weight.


We can link the predictive model to the data using a statistical model. 
In this case we are assuming a normal distribution with some uncertainty 
quantified as $\sigma$. Recall was specific this as: 



$$Y_i \sim Poisson(\mu)$$

where

* $\mu$ is the prediction

The Poisson distribution assumes that the uncertainty is equal to the 
mean. The syntax is similar to the `lm()` function. 



```{r}
fit<- glm(homerange~sex+weight+sex:weight,dat, family="poisson")
```

What is going on with the warnings? That does not give me a warm fuzzy 
feeling about my analysis. Let's see if the model was actually fit. 

```{r}
summary(fit)
```

Well there are estimates for the various parameters of the model. Let's 
be diligent and check the assumptions and make sure things are ok with 
our assumptions. I will start with the plot of residuals versus 
predicted values. 



```{r}
plot(resid(fit)~fitted(fit))
```

I don't go chasing waterfalls that often, but that *does* not look good. 
Or maybe it does? The variance of the Poisson is the mean so you would 
expect some fanning of residuals as the home range increases. 

Now, I hope the model is not misspecified. I will evaluate that by 
looking at a plot of observed versus the fitted values 

```{r}
dat$pred<- predict(fit,dat)
plot(homerange~pred,data = dat)
abline(0,1)
```

That model fit is atrocious. What is going on with the predictions? Why 
are they negative? Well it turns out that the `glm()` function uses a 
log link to model the response. So the predictions are on log scale and 
we need to transform them back to home ranges. The `predict()` function 
does this by specifying the `type='response'` argument. The defaulat is 
`type='link'` 

```{r}
dat$pred<- predict(fit,dat,type='response')
plot(homerange~pred,dat)
abline(0,1)
```

That looks a whole lot better. But I am still a concerned about all the 
warnings that happened during the model fitting. What is up with those. 
Let's look at our data again. 

```{r}
head(dat)
```

Ohhh, I think I see it. The Poisson distribution is only good for 
integers and home range is continuous. I wonder what happens is I round 
my home ranges to the nearest whole number and coerce it to an integer. 


```{r}
dat$hr_int<- as.integer(round(dat$homerange,0))
```

Now let's refit the model and see if the warnings persist.


```{r}
fit<- glm(hr_int~sex+weight,dat, family="poisson")
```

Awesome sauce, no warnings! 
<right>
```{r echo=FALSE, out.width="25%"}
include_graphics("media/awesome-sauce.jpg")
```
</right>


It is always good to look at the model residuals and model fit. 


 
```{r}
dat$resid<-resid(fit)
dat$pred<-exp(fitted(fit))# response link
plot(hr_int~pred,dat)
abline(0,1)
```

The plot of observed versus predicted looks pretty good if you exclude 
values greater than 200. It looks linear which is what we want. Now 
let's look at the residuals. 



```{r}
plot(resid~pred,dat)
```

Ummm, that looks weird. First there are some home ranges that are 
predicted to be 0, or at least close to it. Second, what is up with the 
extreme residuals? Maybe a Poisson was not a great idea. Let's go back 
and look at the data. Recall we coerced home range from a numeric to an 
integer. Maybe that hand an unintended consequence. Let's look at the 
first 10 lines of the data. 




```{r}
head(dat,10)
```

Recall we rounded `homerange` to the nearest integer and then coerced 
that value to an integer `hr_int`. Well that did have an unexpected 
effect. There are many home ranges that are 0. That seems biologically 
improbable for living bears. 
 
What can we do? Well we can maybe convert home ranges to a number that 
is less likely to round to 0. For example we can convert square km to 
square m and then round those values to get whole numbers so the Poisson 
does not yell at us and throw a bunch of warnings again. Let's go for it 
and see what happens. 

```{r}
dat$hr_mm<-dat$homerange*1000000
fit<-glm(hr_mm~sex+weight,dat,family="poisson")
```

Uggg, that didn't work either. Maybe those cool kids were not as cool as 
I thought. You shouldn't assume a variable is has a Poisson distribution 
if it is not an integer. 


## More messing with the distribution


Well, the link function for the Poisson was a log. It has 
some nice properties that when you take the anti-log (i.e., `exp()`) 
then you get a positive value. Let's try it. 

```{r}
exp(-5)
exp(2)
```

Neat, all values greater than 0! That makes biological sense as homerange
should be greater than 0. So what if we simply take the `log()` of the homeranges?

```{r}
dat$lhomerange<-log(dat$homerange)

# plot our transformed data
plot(lhomerange~weight, data= dat,
    xlab="Weight",
    ylab="ln(Homerange)",
    type='n')
points(lhomerange~weight,data=dat,subset=sex=="male",pch=1)
points(lhomerange~weight,data=dat,subset=sex=="female",pch=2)
legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

Well that looks a whole lot better. Let's fit a linear model and see how 
that goes. Formally the linear model, or log-linear model in this case 
is recast as: 


$$log(\mu) = \beta_0 + \beta_1\cdot \text{Sex}+
    \beta_2\cdot\text{Weight}+
    \beta_3\cdot\text{Weight}\cdot\text{Sex} $$

where,

* $\beta_0$ is the intercept of the model
* $\beta_1$ is the effect of sex.
* $\beta_2$ is the effect of weight.
* $\beta_3$ is the interaction of sex and weight.

We can link the predictive model to the data using a statistical model. 
In this case we are assuming a normal distribution with some uncertainty 
quantified as $\sigma$. Recall was specific this as: 



$$log(Y_i) \sim Normal(log(\mu),\sigma)$$

where

* $log(\mu)$ is the log of prediction
* $\sigma$ is the standard deviation.

Note here we are working on log scale so we need to log transform our observed data.

```{r}
fit<- lm(lhomerange~weight+sex+weight:sex,data=dat)
dat$pred<- fitted(fit) # add predictions to dataset
dat$resid<- resid(fit) # add residuals to dataset
```

Let's check the plot of residuals versus predicted values.

```{r}
plot(resid~pred,data=dat)
```

Cool, that looks legit. Now let's check the model structure.

```{r}
plot(lhomerange~pred,data=dat)
abline(0,1)
```

That curvature we saw before is gone, looks linear to me. Now we can get to the 
good stuff. The betas and the uncertainty.

```{r}
summary(fit)
```

Boy that is a pretty good fit!

```{r}
betas<- coef(fit)
betas
sigma<- summary(fit)$sigma
sigma
```

That sigma is a bit high, the sigma in this case is the same as the 
coefficient of variation. Because of the log transformation the variance 
increases with the mean (i.e., predicted value). Let's make some 
predictions using the `predict()` function. Recall we made `preddat` to 
predict values for earlier? 


```{r}
preddat$pred<- predict(fit, preddat)
```

Oh, I can't wait, lets look at our predictions!

```{r}
head(preddat)
```

What is going on? The homeranges are negative? Oh I remember we did a 
log transformation so the predicted values are on log scale. We need to 
take the antilog using the `exp()` function. 




```{r}
preddat$pred<- exp(preddat$pred)
head(preddat)
```

Those values look more like it. Now we can put it all together and see 
what we have going on. 



```{r}

plot(homerange~weight, data= dat,
    xlab="Weight",
    ylab="Homerange(km)",
    type='n')
points(homerange~weight,data=dat,subset=sex=="male",pch=1)
points(homerange~weight,data=dat,subset=sex=="female",pch=2)

points(pred~weight,data=preddat,subset=sex=="male",type='l',lty=1)
points(pred~weight,data=preddat,subset=sex=="female",type='l',lty=2)

legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

Ok, it all looks great. Now we can move on and get our coefficients and 
uncertainty. 

```{r}
betas
sigma
```

Now the key here is that with the interaction of a factor (sex) we need to 
combine the betas so we essentially 2 equations. The model for males is

$$exp(\beta_0 + \beta_1 \cdot \text{Weight})$$


and females is 

$$exp((\beta_0+\beta_2) + (\beta_1+\beta_3)\cdot\text{weight}).$$
 
The parameterized models are: 

* Males: $exp(-15.22+0.102\cdot \text{Weight})$
* Females: $exp(-14.04+0.105\cdot \text{Weight})$



## Linear model misconceptions and assumptions

Couple of common misconceptions I have run into in the understanding of 
linear models 

1. The covariates need to be normally distributed - *FALSE*
2. The response variable needs to be normally distributed - *FALSE* the 
model residuals do though! 

There are some assumptions that need to be addressed though:

1. The response variable is linearly related to the model 
2. The residuals are normally distributed - but this can be modified by 
assuming different distributions 
3. Distributional variances conform to assumptions 

These assumptions can be evaluated by visually inspecting the following 
plots 

1. Plot of response variable (_y_-axis) to predicted variables (_x_-axis)
2. Plot of residuals (_y_-axis) to predicted variables (_x_-axis)

Now that we have a fitted model we can evaluate the assumptions. It is 
usually best to do this before looking at the output. R has some built 
in plots that can be viewed using the code `plot(fit)` but I prefer to 
do a couple of simple plots to assess model assumptions. 

