
















## Frequency elicitation 

Frequency elicitation is an indirect, quantitative method that does not 
require experts that are familiar and comfortable with statistical 
concepts. Typical type of question for frequency elicitation is: 100 
individuals from an amphibian species are exposed to temperatures > 33C, 
how many will survive? These values can be used to calculate 
probabilities and average across experts. For example assume, that 4 
experts answered the above question by providing the following values: 
50, 30, 25, and 40. 


```{r}
n.surv<-c(50, 30, 25, 40) 
#remember 100 individuals 
p.surv<-n.surv/100 
mean(p.surv) 
var(p.surv) 
```


Here we also could use weighting scheme as demonstrated above and use 
the method of moment or maximum likelihood approach to estimate the 
parameters of a beta distribution, if we desired. Alternatively, we 
could use the values directly to estimate the parameters of a beta 
distribution. Remember that the parameters of a beta distribution 
represent: a = number of successes (e.g., survivors) and b = the number 
of losses (e.g. deaths), so we have 


```{r}
n.surv<-c(50, 30, 25, 40) 
n.die<-100- n.surv 
#let's see what that gets us in terms of means and variance for probability 
p.s<-rbeta(1000,n.surv,n.die) 
mean(p.s) 
0.3631856 
var(p.s) 
```

The mean value was fairly close as was the variance. If we wanted to 
weight experts differently, we could rescale the values. For example, 
assume that expert 1 and 2, should receive half the weight of experts 3 
and 4. We could do the following: 


```{r}
n.surv<-c(25, 15, 25, 40) 
n.die<-c(25, 35, 75, 60) 
## notice that the sum of survive and die for expert 1 and 2 is 50 
n.surv+n.die 
p.s<-rbeta(1000,n.surv,n.die) 
mean(p.s) 
var(p.s) 
```

Keep in mind that the variance on the beta is influenced by the size of 
the parameters for example: 


```{r}
p.s<-rbeta(1000,1,1) 
mean(p.s) 
var(p.s) 
#expected value the same, variance different 
p.s<-rbeta(1000,100,100) 
mean(p.s) 
var(p.s) 
```

<!--##########################################-->
Using the above values elicited from the 4 experts, rescale the number 
surviving to correspond to the same proportion assuming 20 individuals 
from an amphibian species are exposed to temperatures > 33C and 1000 
individuals from an amphibian species are exposed to temperatures > 33C. 
e.g., expert 1's values would be 10 and 500, respectively. For each set 
of values, calculate the mean and variance of survival probabilities by 

1. first calculating survival probabilities then estimating the mean and 
variance and 
2. using the number animals surviving and dying with a beta 
random number generator. 

What changed? Does it matter? 

Here are the original numbers.


```{r}
n.surv <- c(50, 30, 25, 40)
```

If we assume there were only 20 animals


```{r}
n.new<- 20*n.surv/100 # originally 100 animals
n.surv<- round(n.new,0)# round to keep to whole animals
n.surv
```

Now we can calculate mean and variance of the survival estimates. 


```{r}
p.surv<-n.surv/20
mean(p.surv)
var(p.surv)
```

We can calculate the number that did not survive easily. 


```{r}
n.die<-20- n.surv
```

Let's see what that gets us in terms of means and variance for 
probability. Remember the beta terms are for the number of successes and 
failures. We can feed in the number surviving and number dead to 
simulate survival probabilities. Note that R recycles the 4 n values, so 
that there are 250 observations for each survival/dead combination. 



```{r}
p.s<-rbeta(1000,shape1=n.surv,shape2=n.die)
mean(p.s)
var(p.s)
```

Lets try the same thing with 1000 animals. 


```{r}
n.surv <- c(50, 30, 25, 40)# we overwrote this above
n.new<- 1000*n.surv/100 # originally 100 animals
n.surv<- round(n.new,0)# round to keep to whole animals
n.surv
n.die<-1000- n.surv
p.surv<-n.surv/1000
mean(p.surv)
var(p.surv)
```

The mean is the same, variance is lower

Again, let's see what 1000 animals gets us in terms of means and 
variance for probability 



```{r}
p.s<-rbeta(1000,n.surv,n.die)
mean(p.s)
var(p.s)
```

Again the mean is the same but the variance is smaller. 

**The important thing to realize here is that simply taking the mean 
and the variance of the estimated survivals does not account for sample 
size.** 

Regardless of sample size the mean was 0.3625 and variance was 0.01229. 
While the simulated values of the mean were the same approximately 0.36 
the variance decreased as the sample size increased from 0.01908 for n = 
20, to 0.009406 for n = 1000. This should make sense as more trials 
reduces the variability in the estimate of the expected (mean) 
survival.* 




## Value elicitation

Value elicitation is a direct, quantitative method that does not 
require experts that are familiar and comfortable with statistical 
concepts. Typical type of question for value elicitation is: 

* What is the LD50 temperature for an amphibian species? * (LD50 is the 
temperature where 50% of animals die). Assume that we received the 
following answer from 6 different experts: 30, 31, 35, 37, 32, 34.5 


```{r}
temp<-c(30, 31, 35, 37, 32, 34.5) 
mean(temp) 
sd(temp) 
```
These parameters can then be used to parameterize relations among model 
components. For example, to estimate the probability that 50% of the 
amphibians will die is temperatures reach 34 degrees: 


```{r}
pnorm(34,mean(temp),sd(temp)) 
```

## Function elicitation 

Function elicitation is an indirect, quantitative method that does not 
require experts that are familiar and comfortable with statistical 
concepts. Here, experts define the functional relationship between two 
or more model components using a graphical representation. For example, 
an expert is asked to define the relation between amphibian survival and 
summer temperatures by drawing a line on paper or in a spreadsheet. For 
example, the figure below was drawn by an expert: 


```{r, echo=FALSE}
expertDat<- data.frame(Ave.summer.temp = c(25,26,27,
    28,29,30,31,32,33,34,
    35,36,37,38,39,40),
Estimated.survival = c(0.85,0.825,0.8,0.75,0.725,
    0.7,0.65,0.6,0.55,0.5,0.45,
    0.4,0.35,0.3,0.275,0.225))
plot(Estimated.survival~Ave.summer.temp,expertDat,
    las=1, 
    ylab="Survival", 
    xlab="Average summer temperature",
    ylim=c(0,1), 
    type='l')

```

This drawn relationship needs to be turned into a function. The first 
step is to estimate the survival values where the cross the summer 
temperatures. Doing so would get you the data contained in the 
`expertDat` below. Because we are modeling a probability, logit-linear 
regression model seems appropriate. However, R requires the response to 
be binary and in live, dead format. For convenience, assume that we 
began with 100 individuals and calculate the number surviving and dying 
based on the probabilities estimates from the figure drawn by the 
expert. We then have: 




```{r}
expertDat<- data.frame(Ave.summer.temp = c(25,26,27,28,
    29,30,31,32,
    33,34,35,36,
    37,38,39,40),
Estimated.survival = c(0.85,0.825,
    0.8,0.75,0.725,
    0.7,0.65,0.6,
    0.55,0.5,0.45,
    0.4,0.35,0.3,
    0.275,0.225))
expertDat
```


```{r}
surv<-round(expertDat$Estimated.survival*100) 
died<-100-surv 
response<-cbind(surv,died) 
glm(response ~ expertDat$Ave.summer.temp, fam=binomial) 
```

 The function for defining this relationship is a logit linear model 
$6.689 - 0.197 \cdot temperature$. To incorporate the variability among 
experts, the slopes and intercepts would be averaged and the averages 
used to parameterize the model. 


### Exercise 3: 

Let's use the graph below created from `musselDat` to estimate a 
function that will close approximate the expert’s belief of the 
relationship between mussel growth and temperature. 


```{r}
musselDat<- data.frame(Ave.summer.temp=c(14,15,16,17,18,19,
20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35),
  Mussel.growth=c(23.314,24.115,24.716,25.317,25.718,
  25.919,26.120,26.121,26.122,25.923,25.524,25.125,24.526,
  23.927,23.128,22.129,21.130,19.931,18.732,17.333,15.734,
  14.135))

plot(Mussel.growth~Ave.summer.temp, 
    data=musselDat, 
    xlab= "Average summer temperature",
    ylab="Mussel growth",
    las=1,
    type='l')
```

We can use the graph below to estimate a function that will close approximate 
the expert’s belief of the relationship between mussel growth and 
temperature. 

The relationship looks nonlinear maybe need a quadratic term 


```{r,eval=FALSE}
mussel.data$Ave.temp.sq<-mussel.data$Ave.summer.temp^2
mod<-lm(Mussel.growth ~ Ave.summer.temp + Ave.temp.sq, mussel.data)
```

Plot the function to make sure it represents the expert figure 


```{r,eval=FALSE}
temp<-c(14:35)
pred<-predict(mod,Ave.summer.temp = temp)
plot(pred~temp,
    lty=1,
    type = "l",
    lwd=1,
    ylab="Mussel growth",
    xlab="Average temperature",
    las=1)
points(Mussel.growth~Ave.summer.temp,mussel.data)
legend("topright",c("Predicted","Elicited"),pch=c(NA,1),lty=c(1,NA))
```



## Quantifying uncertainty in expert judgment

To minimize the effect of overconfidence, it is recommended to use the 
four step process to uncertainty elicitation proposed by Speirs-Bridge 
et al. (2010). In the 4 step process, experts are asked the following 4 
questions: 
 

1. What do you think the lowest value could be? ____
2. What do you think the highest value could be? ____ 
3. What is your most likely estimate?___ 
4. How confident (%) are you that the interval you created, from lowest highest, will capture the true value? ___ 


For example, 100 individuals from an amphibian species are exposed 33C 
temperatures. 


1. What are the fewest number that would survive? ____ 
2. What are the most that would survive? ____ 
3. What is your most likely estimate of the number surviving?___ 
4. How confident (%) are you that the interval you created, from lowest highest, will capture the true value? ___


Let’s assume that we asked an expert the first question above: Given 
that the maximum summer water temperature is 33C or greater, what is the 
probability that an amphibian species will survive 


1. What do you think the lowest value could be? **0.25**
2. What do you think the highest value could be? **0.9**
3. What is your most likely estimate?  **0.55**
4. How confident (%) are you that the interval you created, from lowest highest,will capture the true value? **0.9**


To estimate the parameters, we need to find the beta distribution that 
best fits those conditions, i.e., a median value of 0.5 and upper and 
lower 90% confidence limits of 0.25 to 0.9. WE can fit these parameters 
in R with the following code: 


```{r,warning=FALSE,message=FALSE}
## lower, median, and upper values 
w<-c(0.25, 0.55, 0.9) 
### load library first 
library(fitdistrplus) 
# 90% confidence so specify lower 5 and upper 95 percentiles 
est<-qmedist(w, "beta", probs=c(0.05, 0.95)) 
parms<-est$estimate 
parms 
## let’s see how close we got 
test<-rbeta(1000,parms[1],parms[2]) 
quantile(test,c(0.05,0.5,0.95)) 
```

Not too bad considering expert’s judgments don’t always follow the 
laws of probability. Note that shape1 is alpha and shape2 is beta for 
the beta distribution. We could combine these across experts using the 
Bayesian methods we used earlier. 


Let’s try the same thing, but this time asking: What is the LD50 
temperature for an amphibian species? 

1. What do you think the lowest value could be? **33**
2. What do you think the highest value could be? **39**
3. What is your most likely estimate?**35**
4. How confident (%) are you that the interval you created, from lowest highest, will capture the true value? **0.8**

The corresponding R code and output are:


```{r,warning=FALSE,message=FALSE}
## lower, mean, and upper values 
w<-c(33, 35, 39) 
library(fitdistrplus) 
# 80% confidence so specify lower 10 and upper 90 percentiles 
est<-qmedist(w, "norm", probs=c(0.10, 0.90)) 
```

Don't worry about the warnings if it throws them, the functions is just 
being evaluated at some boundary locations 




```{r}
parms<-est$estimate 
parms 
## lets see how close we got 
test<-rnorm(1000,parms[1],parms[2]) 
quantile(test,c(0.10,0.5,0.90)) 
```

As before, the estimates are not too bad and they can be combined across 
experts using prior and posterior techniques we learned in lab 6. 


## Eliciting maximum temperature

You need to ask 2 friends the following question: What will be the 
maximum temperature tomorrow? Use the 4 step process to obtain their 
estimates and levels of confidence. Then estimate the parameters for a 
normal distribution using the above process in R. Combine the estimates 
for the two experts using the Bayesian prior and posterior approach 
detailed in Lab 6. Note the sample size you should use to calculate the 
posterior values should be 1, (i.e., n=1). 


For this example our data is as follows:

| Value                	| Expert 1 	| Expert 2 	|
|----------------------	|:--------:	|:--------:	|
| Lowest value         	|    50    	|    50    	|
| Highest value        	|    75    	|    63    	|
| Most likely value    	|    65    	|    60    	|
| Confidence level (%) 	|    99    	|    98    	|

Note: Your data will be different here.  

```{r,warning=FALSE,message=FALSE}
w<-c(50,65,75)
c<-0.99 # CONFIDENCE
CI<-c((1-c)/2,1-(1-c)/2) # CONFIDENCE INTERVALE
est<-qmedist(w,"norm",probs=CI) # WE CAN 
#Estimates
parms1<-est$estimate
parms1
test<-rnorm(1000,parms1[1],parms1[2])
quantile(test,c(0.005,0.5,0.995))
```

```{r,warning=FALSE,message=FALSE}
#lower, median, and upper values from Expert 2
w<-c(50,60,63)
c<-0.98
CI<-c((1-c)/2,1-(1-c)/2)
est<-qmedist(w,"norm",probs=CI)
#Estimates
parms2<-est$estimate
parms2
test<-rnorm(1000,parms2[1],parms2[2])
quantile(test,c(0.01,0.5,0.99))
```

Note: The NAs produced are just part of the optimization. 


We can combine the estimates for the two experts using the Bayesian 
prior and posterior approach detailed in Lab 6. Note the sample size you 
should use to calculate the posterior values should be 1, (i.e., n=1). 


Here are the estimates for expert 1 which is used as the prior. 




```{r}
mu_0<-parms1[1]
s_0<-parms1[2]
mu<-seq(0,90,1)
prior<-dnorm(mu,mu_0,s_0)
plot(mu,prior,col="red",lty=2,lwd=3,type='l')
```

Now we add the data from expert 2 and update the posterior. 


```{r}
#Addition of second expert
n<-1
xbar<-parms2[1]
sd<-parms2[2]
v_1<-(1/s_0^2+n/sd^2)^(-1)
mu_1<-(mu_0/s_0^2+n*xbar/sd^2)*v_1
s_1<-sqrt(v_1)
post<-dnorm(mu,mu_1,s_1)
```

Lets look at this and see what the prior (expert 1 data only) and 
posterior (expert 1 and expert 2's data). 


```{r}
plot(mu,post,xlim=c(20,90),type='l',lwd=3)
lines(mu,prior,col="red",lty=2,lwd=3)
legend("topleft",c("Prior","Posterior"), lty=c(2,1),col=c("red","black"))
```

The mean and variance are:

```{r}
mu_1
s_1
```

