---
title: ""
---
```{r echo=FALSE, out.width="95%"}
include_graphics("media/banner-07
.jpg")
```

# Predicting outcomes in a decision context: generalized linear models

<!--source progress code-->
```{r child='in-progress.Rmd', eval=TRUE}
``` 



# Overview

# Objectives 

By the end of this tutorial you should be able to:

1. Understand linear models 
1. Understand interactions in linear models
2. The effect of varying distributions 
3. Using model selection for inference

# Background and the data

Despite current debates about which bear is best [1](https://youtu.be/N8d86Kjl1dg), we will be modeling
 black bear home ranges using a linear model.
Linear and generalized linear models are essentially the same thing.
There is a response variable, home range in this case.
Variability in home range is then explained using a linear combination of covariates.
Formally, this is expressed as:

$\hat{Y_i} = \beta_{0} + \beta_{1}\cdot X_{i} + \epsilon$

where Y is home range in square kilometers, $\beta_{0}$ is the model intercept, $\beta_{1}$ is the
 effect of covariate $X$ on home range size, $i$ indexes individual bear, and $\epsilon$ is the error.
The parameter $\epsilon$ is a normally distributed value with mean 0 and some level of variability
 represented as a standard deviation.  
Covariates in a linear model can  be categorical (e.g., sex, stage) or continuous (e.g., weight).  
The model can also include an interaction of covariates.


Preliminaries done, let's look at the data.
But first we need to read it in.


```{r}
setwd('C:/Users/mcolvin/Google Drive/Tutorials/LM-Versus-GLM')
dat<-read.csv("./output/homerange-data.csv")
```

Let's make sure things look good.

```{r}
head(dat)
```

and check the types of data that are read in

```{r}
str(dat)
```

Everything looks good.
The sex variable is a factor, as it should be.
The weight and the home range variable is a numeric.

```{r}
boxplot(homerange~sex,dat)

hist(dat$homerange)
```

Ugggh. That does not look normal to me, but let's go with it and fit a linear model and see what happens.


# 1. A basic linear model of a normally distributed variable


Let's fit the model to some the data.
Suppose we are interested in evaluating whether female bears have larger or smaller home ranges than
 male bears.
Now, I crafted this comparison deliberately to refine some understanding of R and linear models.
First we need to examine the default way R treats categorical variables, like sex.
We can figure out which factor is 'first' using the `levels()` function. 

```{r}
levels(dat$sex)
```

Female is listed first then male.
By default, R orders levels of categorical variables alphabetically.
Let's see how it is treated in a linear model.

```{r}
fit<-lm(homerange~sex,dat) # fit the model
summary(fit) # model summary
coef(fit) # extract estimated coefficients
```

The estimate of the intercept is `r round(coef(fit)[1],3)` and the effect of sex is 
 `r round(coef(fit)[2],3)`.
Notice the name of the effect of sex is called `sexmale`.
This is the effect of being male on home range size.
The intercept represents the expected home range of a female bear.
The expected home range of a male bear is then `r round(coef(fit)[1],3)`+ `r round(coef(fit)[2],3)` =
 `r round(coef(fit)[1],3) + round(coef(fit)[2],3)` km, given this model.
Recall we were interested in whether female bears had a larger or smaller home range.
While you can evaluate with the model we fit, switching the order so male is the first level
 and would therefore be the intercept, would make sense.
The second estimated coefficient is then the effect of being female.
To do this we need to reorder the factor levels in R which can be done with the `factor()` function.

```{r}
dat$sex<- factor(dat$sex,levels=c("male","female")) # reorder factor levels
``` 
 
Now let's refit the same model and see what happens now.

```{r}
fit<- lm(homerange~sex,dat)
summary(fit) # model summary
coef(fit) # estimated coefficients
```

Now the expected homerange for a male bear is the intercept and a female bear is `r round(coef(fit)[1],3)`+ `r round(coef(fit)[2],3)` =
 `r round(coef(fit)[1],3) + round(coef(fit)[2],3)` km, given this model.
Compare the expected home range for a female bear with the intercept for the first model fit, the are
 the same.
Formally, the model estimates 3 parameters:

* $\beta_{0}$ = `r round(coef(fit)[1],3)`; this is the average home range of a male bear
* $\beta_{1}$ = `r round(coef(fit)[2],3)`the effect of being a female on home range
* $\sigma$ = `r round(summary(fit)$sigma,3)`residual standard error

Ok, now we also suspect there is an effect of weight on home range, larger bears have large home
 ranges.
The effect of weight can be evaluated by adding weight to the linear model.
The model parameters for this model are the same as before but there is a new parameter estimated,
 $\beta_{2}$ which is the effect of weight on home range.  
Let's fit the model.

```{r}
fit<-lm(homerange~sex+weight,dat)
summary(fit)
coef(fit)
```

Well I'll be the model fits pretty well.
The r-squared value is `r summary(fit)$r.squared` which is pretty good for ecological data, right?
Either way the parameter estimates for the model are:

* $\beta_{0}$ = `r round(coef(fit)[1],3)`; this is the average home range of a male bear
* $\beta_{1}$ = `r round(coef(fit)[2],3)`the effect of being a female on home range
* $\beta_{2}$ = `r round(coef(fit)[3],3)`the effect of weight on home range
* $\sigma$ = `r round(summary(fit)$sigma,3)`residual standard error

Neat, we have fit a model that estimates home range given bear sex and weight.
We can use the model to predict home ranges for male and female bears of varying weights.
Let's do that and see how good our model works, the anticipation is killing me!
First we need a dataset of male and female bears to predict for.
Bear weights vary from `r min(dat$weight)` to `r max(dat$weight)` and therefor we should predict
 between those values.
Above or below those values you are getting into the realm of extrapolation.

```{r}
newdat<- data.frame(weight=seq(min(dat$weight), max(dat$weight),1))
head(newdat)
```

But wait, don't we need sex in the `data.frame` too?
Right you are, let's add it.
We can do this by 

```{r}
newdat$sex<-"male"
preddat<- newdat
newdat$sex<-"female"
preddat<- rbind(preddat,newdat)
```

Let's double check the data to make sure it is formatted correctly.

```{r}
str(preddat)
```

Hmmmm, that is not quite right.
Sex is a character, it should be a factor.
Let's fix that.

```{r}
preddat$sex<- factor(preddat$sex,levels=c("male","female"))
levels(preddat$sex)
```
 
Ok, that looks better, especially the ordering of the levels.
Now back to the fun stuff, let's get predicting with teh `predict()` function.

```{r}
preddat$homerange_hat<- predict(fit,preddat)
head(preddat)
```
Now we can plot the predictions.

```{r}
plot(homerange_hat~weight,preddat,type='n',xlab="Weight",ylab="Home range (km)")
points(homerange_hat~weight,preddat,subset=sex=="male",type='l',lty=1)
points(homerange_hat~weight,preddat,subset=sex=="female",type='l',lty=2)
legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

Ok, I am not a rocket surgeon, but that does not seem right, how can home ranges be negative?
Let's plot it with the original data.


```{r}
plot(homerange~weight,dat,type='n',xlab="Weight",ylab="Home range (km)",ylim=c(-1,10))
points(homerange~weight,dat,subset=sex=="male",pch=1)
points(homerange~weight,dat,subset=sex=="female",pch=2)

points(homerange_hat~weight,preddat,subset=sex=="male",type='l',lty=1)
points(homerange_hat~weight,preddat,subset=sex=="female",type='l',lty=2)
legend('topleft',legend=c("Males","Female"),pch=c(1,2))
```

That is a terrible fit.
But, the r-squared value was `r summary(fit)$r.squared`?
I probably should have looked at the data first, but that is part of the point of this example.
There are a couple of things that might be going on here.
First, the distribution could be wrong.
That data does not look all the normally distributed.
Second, the model could be wrong and home ranges are not linearly related to sex and weight.
We can evaluate these 2 problems with plots of residuals versus predicted to evaluate distribution
 and observed versus predicted values to evaluate if the model is misspecified.
Here we go.

```{r}
dat$resids<-resid(fit)
dat$preds<-fitted(fit)

# plot of residuals versus predicted
# should be normal and centered around 0
plot(resids~preds,dat)
abline(h=0)
```

Well that does not good at all.
The residuals are not centered around 0, the model tends to over predict at low values, under
 predict at intermediate values, and over predict at high values.
 

```{r}
# plot of observed versus predicted
# should fall on a 1:1 line
plot(homerange~preds,dat)
abline(0,1)
```

That does not look good at all either.
The model appears to be misspecified and some unaccounted for curvature in the model.
Let's tackle the distribution issue to see if that fixes things.

### Messing with the distribution

Some cool kids told me that a Poisson might be the way to go because it models things 0 or greater.
Home ranges are greater than 0 and it makes good sense to not let home range dip below 0.
A linear model with a Poisson distribution can be fit using the `glm()` function.
The syntax is similar to the `lm()` function.

```{r}
fit<- glm(homerange~sex+weight,dat, family="poisson")
```

What is going on with the warnings?
That does not give me a warm fuzzy feeling about my analysis.
Let's see if the model was actually fit.

```{r}
summary(fit)
```

Well there are estimates for the various parameters of the model.
Let's be diligent and check the assumptions and make sure things are ok with our assumptions.
I will start with the plot of residuals versus predicted values. 

```{r}
plot(resid(fit)~fitted(fit))
```

I don't know much, but that *does* not look good.
Or maybe it does?
The variance of the Poisson is the mean so you would expect some fanning of residuals as the home range increases.

Now, I hope the model is not misspecified.
I will evaluate that by looking at a plot of observed versus the fitted values

```{r}
dat$pred<- predict(fit,dat)
plot(homerange~pred,dat)
abline(0,1)
```
That model fit is atrocious.
What is going on with the predictions?
Why are they negative?
Well it turns out that the `glm()` function uses a log link to model the response.
So the predictions are on log scale and we need to transform them back to home ranges.
The `predict()` function does this by specifying the `type='response'` argument.
The defaulat is `type='link'`

```{r}
dat$pred<- predict(fit,dat,type='response')
plot(homerange~pred,dat)
abline(0,1)
```

That looks a whole lot better.
But I am still a concerned about all the warnings that happened during the model fitting.
What is up with those. 
Let's look at our data again.


```{r}
head(dat)
```

Ohhh, I think I see it.
The Poisson distribution is only good for integers and home range is continuous.
I wonder what happens is I round my home ranges to the nearest whole number and coerce it to an integer.


```{r}
dat$hr_int<- as.integer(round(dat$homerange,0))
```

Now let's refit the model and see if the warnings persist.


```{r}
fit<- glm(hr_int~sex+weight,dat, family="poisson")
```
Awesome sauce, no warnings!
<IMG SRC="figures/awesome-sauce.jpg" ALIGN="right" width=25%/>
It is always good to look at the model residuals
 and model fit.
 
```{r}
dat$resid<-resid(fit)
dat$pred<-exp(fitted(fit))# response link
plot(hr_int~pred,dat)
abline(0,1)
```
The plot of observed versus predicted looks
 pretty good if you exclude values greater
 than 200.
It looks linear which is what we want.
Now let's look at the residuals.

```{r}
plot(resid~pred,dat)

```

Ummm, that looks weird.
First there are some home ranges that are
 predicted to be 0, or at least close to it.
Second, what is up with the extreme residuals?
Maybe a Poisson was not a great idea.
Let's go back and look at the data.
Recall we coerced home range from a numeric
 to an integer.
Maybe that hand an unintended consequence.
Let's look at the first 10 lines of the data.


```{r}
head(dat,10)
```
Recall we rounded `homerange` to the nearest
 integer and then coerced that value to an 
 integer `hr_int`.
Well that did have an unexpected effect.
There are many home ranges that are 0.
That seems biologically improbable for living
 bears.

 
What can we do?
Well we can maybe convert home ranges to a
 number that is less likely to round to 0.
For example we can convert square km to square
 m and then round those values to get whole
 numbers so the Poisson does not yell at us
 and throw a bunch of warnings again.
Let's go for it and see what happens.

```{r}
dat$hr_mm<-dat$homerange*1000000
```
fit<-glm(hr_mm~sex+weight,dat,family="poisson")




```{r}

```



* look at data, got some 0s
* can rescale to get integers
* what about over dispersion?
* log link -> log normal
* Model selection





### Misconceptions and assumptions

Couple of common misconceptions I have run into in the understanding of linear models

1. The covariates need to be normally distributed - *FALSE*
2. The response variable needs to be normally distributed  - *FALSE* the model residuals do though!

There are some assumptions that need to be addressed though:

1. The response variable is linearly related to the model
2. The residuals are normally distributed - but this can be modified by assuming different distributions
3. Distributional variances conform to assumptions

These assumptions can be evaluated by visually inspecting the following plots

1. Plot of response variable (_y_-axis) to predicted variables (_x_-axis)
2. Plot of residuals (_y_-axis) to predicted variables (_x_-axis)


Now that we have a fitted model we can evaluate the assumptions.
It is usually best to do this before looking at the output.
R has some built in plots that can be viewed using the code `plot(fit)` but I prefer to do a couple
 of simple plots to assess model assumptions.

```{r}
dat$pred<- fitted(fit) # add predictions to dataset
dat$resid<- resid(fit) # add residuals to dataset
plot(homerange~pred,dat)
plot(resid~pred,dat)
```

# Distribution-normal, lognormal, and poisson


* Normal
* Log normal

# A linear model with a interaction

Let's add some additional biological realism to this example.  
Specifically, in this example male bears have larger home ranges than females and the larger bears
 have larger home ranges.


```{r}
beta_0 <- 0.3 # the intercept
beta_1 <- 1.2 # the effect of being female
beta_2 <- 0.3 # the effect of weight 
beta_3 <- 0.2 # the interaction of sex and weight
epsilon <- 1 # error 

```


# The actual model and data generating code

The example above was simulated using known parameters from the code below.
Using the code below you can see that true model that generated the dataset was the one that included
 a sex * weight interaction that allows the effect of weight on homerange to vary by sex.  
Ideally, model parameter estimates are the same as those used to generate the data, at least close.
Additionally, the model selection should weight the generating model relatively high relative to
 competing models.
