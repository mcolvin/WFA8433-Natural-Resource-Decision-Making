---
title: ""
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true   
    collapsed: false
---

 <!--


source("_build.R")
build("Class-18",bld="PAGE",docs=FALSE) # bld = PAGE,ENTIRE,SCRIPT

# PURL THIS SHIT & MOVE FILES TO DOCS
build("Class-18",bld="SCRIPT",docs=TRUE) # bld = PAGE,ENTIRE,SCRIPT

source("_build.R");build("Class-18",bld="PAGE",docs=TRUE)# bld = PAGE,ENTIRE,SCRIPT

-->


```{r echo=FALSE, out.width="100%"}
include_graphics("media/banner-03.jpg")
rm(list=objects())
```


# Class 18: Estimating abundance using N-Mixture models {-}

# Class preliminaries

* Supplemental background reading for next class(es):
    * Conroy and Peterson Chapter 6 and 7.
    * Powell and Gale Chapter 17.
* Assignment due: None
* Class project: 
    * Be developing your decision model
    * Final exam period-April 28th at 3pm.
* Link to class recording  [YouTube]()
* Today's R script [Class-17.R](scripts/Class-17.R)

## Class overview & objectives 

1. Review final project
1. Review decision models
    1. Influence models
    2. Sensitivity analysis
    3. Response profiles
1. Estimating abundance: _N_-Mixture models

## Getting ready to go

* The R scipt for class can be found [here](scripts/Class-17.R)
* Once you have the script where you want it it where you want open the 
R script and be sure to check the working directory `getwd()` and make 
sure it is where your folder is. 
* If your working directory is not correct, you can set it in Rstudio: 
"Session --> Set Working Directory --> To source file location". Or you 
can use the `setwd()` in the console 
* The data used today can be downloaded here [Rdata](study-area.Rdata)
* You will want to install the following packages

```{r}
#install.packages("reshape2")
#install.packages("unmarked")
#install.packages("fields")
```

```{r,echo=FALSE, eval=FALSE}
dat<-read.csv("tornado.csv")
par(mar=c(3,10,1,1))
plot(0,0,type='n',
    xlim=range(unlist(dat[,c(2,3)])),
    ylim=c(1,7),yaxt='n',xlab="",ylab="")
segments( dat$Minimum,dat$id,  dat$Maximum,dat$id,lwd=4)
axis(side=2, at=c(1:7), labels=dat$Node,las=1)

dat<- read.csv("response-profile.csv")
plot(dat$Crabtree.Creek,xaxt='n',xlab="Number to outplant",
    ylab="Marginal gain", las=1,type='l',ylim=c(78.5,79),lwd=2)
points(dat$MF.Paddy.s,lty=2, type='l',lwd=2)
points(dat$Thomas.Creek,lty=3, type='l',lwd=2)
legend("topleft", c("Crabtree Creek","MF Paddy's","Thomas Creek"),
    lty=c(1,2,3),bty='n')
axis(side=1, at=c(1:6),labels=dat[,1])
```


# N-Mixture models

## Overview

Traditionally metrics like catch per unit effort (CPUE) would be used 
for comparisons (i.e., is cpue higher in one habitat relative to 
another). Comparing CPUE requires many assumptions as it relates to 
catchability ($q$), where Catch is $C = q\cdot f \cdot abundance$. 
Recent advances in _N_-mixture models relaxes this assumption by 
estimating capture probability for each site. One thing to note is that 
capture probability and catchability are not the same thing. Caveats 
aside the _N_-mixture provides a method to estimate density, accounting 
for imperfect capture. The gist of the approach is to repeatedly sample 
a site. The number of critters captured is a function of the underlying 
density and capture probability. For example if there were 100 critters 
at a site and your gear had a capture probability of 0.8 and then you 
sampled that site 5 times, you would expect to capture approximately 80 
critters each time. In reality the data might look like this: 81 74 85 
82 70,this is essentially a capture history but with counts instead of 
0s and 1s. Because there was temporal replicates one can estimated a 
capture probability. 


```{r}
lambda<- 5
```

```{r}
kernalsPerBag<- rpois(6,lambda)
```

Our true number number of kernals 
in bags 1 to 6 were 4, 5, 6, 4, 11,and 1 respectively.


```{r}
ourData<- matrix(c(0,1,0,1,2,0,
    3,2,3,2,2,0,
    2,3,4,4,4,1,
    2,3,1,1,4,1,
    1,4,3,1,5,1,
    1,2,5,1,5,0,
    2,2,3,3,6,0,
    2,4,1,4,5,0,
    0,1,5,0,5,0,
    2,1,1,1,2,0,
    2,2,1,2,6,0,
    2,2,1,2,2,1,
    3,3,3,2,6,1,
    0,1,0,2,2,0,
    2,1,2,0,2,0),ncol=6,nrow=15,byrow=TRUE)
# TRANSPOSE THE DATA TO HAVE 'VISITS' AS COLUMNS   
# AND SITES AS ROWS 
ourData<-t(ourData) 
```



As it relates to a study area, the model may give the ability to estimate 
the density of hard to capture critters and the ability to compare among 
habitats. The design is not that different from what would occur for 
typical fishery or wildlife surveys, sample sites are randomly selected 
within an habitat and then repeated sampling is conducted to get the 
count history. 


## Application to estimating abundance

### The study area

Suppose there is an area with a spatial domain, study area, that looks like 
the image below. The blue represents depth (darker = deeper). 


```{r,echo=FALSE}
## make a dataset
x<- c(1,2,3,4,5,6,5,4,3,2,1)
y<- c(1,3,6,7,9,8,1,1,2,0,1)
thalx<-seq(-10,10,0.1)
slp<- 2.2
thaly<- -3 + slp*thalx
xx<- as.matrix(expand.grid(
	x=seq(0,10,0.2),
	y=seq(0,10,0.2)))
pip<-mgcv::in.out(as.matrix(cbind(x,y)),xx)

irc<- as.data.frame(xx[pip,])
names(irc)<-c("x","y")
irc$siteId<- c(1:nrow(irc))
irc$depth<- sapply(1:nrow(irc),function(x)
	{
	a=-3
	b=2.2
	x0<-irc$x[x]
	y0<-irc$y[x]
	val<-(((x0+b*y0-b*a)/(b^2 + 1))-x0)^2 + (b*((x0+b*y0-b*a)/(b^2+1))+a-y0)^2
	d<- sqrt(val)
	return(d)
	})
irc$depth<- exp(1.5 + -1.75*irc$depth)
z<- reshape2::dcast(irc,x~y,value.var="depth")
x<-z[,1]
y<-as.numeric(names(z[-1]))
z<- as.matrix(z[,-1])
sa<-irc
## end make dataset
save(sa,x,y,z, file="study-area.Rdata")
```

```{r,message=FALSE,warning=FALSE}
load("study-area.Rdata")
library(fields)
library(reshape2)
image.plot(x,y,z,xlim=c(0,7),
    xlab="X",ylab="Y",
    ylim=c(0,9), 
    col=rainbow(n=20,start=3/6,end=4/6),
    asp=1)	
```

Suppose that within the polygon above the true density of critters is 4  
per square meter (i.e., $\lambda = 4$ in _N_-mixture jargon). Applying 
that density to the polygon, the one realization of teh density of critters 
may look like the figure below. Recall we are using rpois so this is one 
stochastic realization.  


```{r,echo=FALSE}
lambda <- 4 # UNDERLYING DENSITY
set.seed(1234) # for reproducibility
sa$abundance<- rpois(nrow(sa),lambda)
z<- dcast(sa,x~y,value.var="abundance")
x<-z[,1]
y<-as.numeric(names(z[-1]))
z<- as.matrix(z[,-1])
image.plot(x,y,z,
    xlim=c(0,7),
    xlab="X",ylab="Y",
    ylim=c(0,9),
    col=heat.colors(n=20),
    asp=1)
```

Using the framework, a survey can sample sites within the habitat. 
Suppose 20 randomly selected sites were used. That would look like the 
image below. 

```{r,echo=FALSE}
nsamples<- 20
sa$siteId<- c(1:nrow(sa))
indx<- sample(1:nrow(sa),nsamples)
sampleSites<- sa[indx,]
image.plot(x=x,y=y,z=z,
    xlim=c(0,7),
    xlab="X",
    ylab="Y",
    ylim=c(0,9),
    col=heat.colors(n=20),
    asp=1)
points(y~x,sampleSites, pch=3,col="black")
legend("topleft",legend="Sample Site",pch=19,bty='n')
```

## Estimating abundance

Using the 20 samples abundance can be estimated using an _N_-mixture 
model, given some assumptions. Variables estimated by the _N_-mixture 
model are: 

* $\lambda$ is initial population density
* $p$ is capture probability

Specifically the process model is:

$$N_{i} \sim Poisson(\lambda)$$

and 

$$y_{i,k}~binomial(N_{i},p)$$

and

$$log(\lambda) = \beta_0$$

and 

$$logit(p) = \gamma_0$$

where

* $N_{i}$ is the predicted count,  
* $\lambda$ is the underlying density,  
* $y_{i,t}$ is the number of critters observed at site $i$ at time $t$,  
* $p$ is capture probability,  
* $i$ indexes each randomly selected site,   
* $k$ indexes each visit,  
* $\beta_0$ is the intercept of the linear model predicting the 
log abundance, and  
* $\gamma_0$ is the intercept of the linear model predicting
the log odds of capture probability. 

The assumptions for the model above included:

* Counts are independent among site $i$ and visit $k$,
* Capture probability is homogenous, and
* $N$ is the true underlying count at site $i$
* Populations closed between surveys


### No covariates: $\lambda$ and $p$ homogenous

The sampling design in the plot above illustrates how the design might 
play out in an IRC habitat. Each site would be visited 2 or more times 
either within a day or within a time period that is sufficiently short, 
such that demographic closure can be assumed. The process assumes 
temporal replicates, however spatial replicates have been used with 
occupancy models, so it is suggestive that it might work for _N_-mixture 
models. This first analysis assumes there is no underlying relationship 
with abiotic or biotic covariates such as depth on abundance or capture 
probability, i.e., density arises from a Poisson process. 


The code below generates catches at 20 sites over 5 occasions given the 
abundance at the site and a capture probability ($p$) = 0.4. 



```{r}

nsamples<- 50 # i = 1,2,3,...20
beta_0<- 1.386 # UNDERLYING DENSITY
gamma_0<- -0.405 # LOG ODDS CAPTURE PROBABILITY

# TRANSFORM TO REAL VALUES
lambda <- exp(beta_0)
lambda
p<- exp(gamma_0)/(1+exp(gamma_0) )
p

# SIMULATE ABUNDANCES 
set.seed(1985)# FOR REPRODUCABILITY; LAST YEAR DLR WAS IN VAN HALEN
sa$N<- rpois(nrow(sa),lambda)

# GENERATE CAPTURE HISTORIES
visits<-5 # k = 1,2,3,4,5
# MATRIX TO HOLD VALUES
y<- matrix(0,nrow(sa),visits) # matrix for all possible sites
for(i in 1:nrow(sa))
	{
    for(k in 1:visits)
        {
        y[i,k]<- rbinom(1,sa$N[i],p)#obs count for visit k and site i
        }
	}
```

The simulated counts at each of the `r nsamples` sites for `r visits` visits is shown 
below. 

```{r}
sample_indx<- sample(1:nrow(sa),nsamples,replace=FALSE)
obs<- y[sample_indx,]
obs
``` 

The data in the table above is then used to estimate $\lambda$ and $p$ 
using the _N_-mixture model. This is done using the `pcount()` function 
from the `unmarked` library. Note there was some manipulation of the 
data using the `unmarkedFramePCount()` function to process the input 
data prior to fitting the model. 

```{r,message=FALSE, warning=FALSE}
# Prepare data
library(unmarked)
data <- unmarkedFramePCount(y = y)

# ~DETECTION ~ ABUNDANCE
fit <- pcount(~1 ~ 1, 
    data=data, 
    K=50) # SET THIS HIGHER THAN YOUR EXPECTED ABUNDANCE
summary(fit)
```

Once the model is fit and estimates are returned, the estimates are then 
back transformed from log and logit scale for abundance and capture 
probability respectively. 

```{r}
# Density
lambda
exp(coef(fit)[1]) # should be close to lambda

# Capture probability
p
exp(coef(fit)[2])/(1+exp(coef(fit)[2])) # should be close p
```
After fitting the _N_-mixture model the estimate of $\lambda$ was `r 
round(exp(coef(fit)[1]),2)`, recall that the value used to generate the 
data was `r lambda` and the estimate of $p$ was `r 
round(plogis(coef(fit)[2]),2)`, recall the that value used was `r p`. 
The estimates should be in the ballpark. 


<!--
need to clarify notation and notation within code
-->

### Covariates for $\lambda$ and $p$


The no covariates for $\lambda$ and $p$ is a rather unrealistic 
condition in nature. This example shows how a biologically relevant 
metric like depth can be used to model abundance and capture 
probability. Note that this example assumes there is a true underlying 
relationship with the abundance and capture probability of the critter
and depth. Recall the figure above of the hypothesized study area 
with varying depths. Suppose abundance was inversely related to 
depth within and study area and that relationship is illustrated below. 
Formally we change the equation predicting $\lambda$ to 

$$log(\lambda) = \beta_0 + \beta_1 \cdot Depth$$

where 

* $\lambda$ is the underlying density,
* $\beta_0$ is the intercept of the linear model predicting the 
density, and  
* $\beta_1$ is the effect of depth on abundance.  


```{r,echo=FALSE}
beta_0 <- 3
beta_1 <- -0.5

# GET A FEEL FOR THE RELATIONSHIP 
depth <- seq(0,max(sa$depth),0.1)
expectedDensity<- exp(beta_0+beta_1*depth)
plot(depth, # VECTOR OF X VALUES
    expectedDensity,# VECTOR OF Y VALUES
    xlab="Depth",
    ylab="Expected density",
    type='l',
    lwd=2,
    las=1)
```

The effect of depth on abundance within the study area is negative, indicating 
critter abundances are higher at lower depths. This underlying density
relationship might look like the figure below where the expected density is 

$$N \sim Poisson(exp(\beta_{0} + \beta_{1} \cdot depth))$$, 

where 
* $\beta_{0} =$ `r beta_0`, and 
* $\beta_{1} =$ `r beta_1`. 


The actual abundance for each site given the depth is simulated and illustrated 
below. 


```{r,echo=FALSE}
sa$abundance <- rpois(nrow(sa),exp(beta_0+beta_1*sa$depth))
plot(abundance~depth,sa,ylab="Abundance",xlab="Depth",las=1)
nsamples<- 40
indx<- sample(1:nrow(sa),nsamples)
sampleSites<- sa[indx,]
```



```{r,echo=FALSE}
gamma_0<- 1
gamma_1<- -0.5
y<- gamma_0+gamma_1*depth
p<- exp(y)/(1+exp(y))
```


Similarly, capture probability can be related to depth or some other 
environmental covariate. Suppose that capture probability was a function 
of depth, this might be the case when using a seine and it might be very 
difficult to fish the deep areas, just a hypothetical example here. This 
relationship might look like the figure below where the capture 
probability is 

$$logit(p) = \gamma_{0} + \gamma_{1} \cdot depth$$ 

where 

* $\beta_{0} =$ `r gamma_0`, and 
* $\beta_{1} =$ `r gamma_1`. 



```{r,echo=FALSE}
plot(depth,p,xlab="Depth",
    ylab="Capture probability",
    las=1,
    type='l')
```


Using the relationship of $p$ and depth and the abundance at each site 
the capture histories can be generated accounting for site-specific 
depth and capture probability. 




```{r}
# GENERATE CAPTURE HISTORIES
visits<-5
p<- exp(gamma_0+gamma_1*sampleSites$depth)/
    (1+exp(gamma_0+gamma_1*sampleSites$depth))
y<- matrix(0,nsamples,visits)
for(i in 1:nsamples)
	{
	y[i,]<- rbinom(visits,sampleSites$abundance[i],p[i])
	}
```


The simulated catch at each of the 40 sites for 5 occasions is shown 
below. 

```{r, echo=FALSE}
y
``` 


The model is fit as before, but with 4 estimates produce 2 intercepts 
and 2 betas for the the relationship of depth on $\lambda$ and $p$. 



```{r}
# PREPARE DATA
data <- unmarkedFramePCount(y = y,
    siteCovs=data.frame(depth=sampleSites$depth))
# FIT THE MODEL WITH DEPTH AS A COVARIATE FOR LAMBDA AND P
fit <- pcount(~depth +1 ~depth+1, 
    data=data, 
    K=150)
fit
```



After fitting the _N_-mixture model the estimate of $\beta_{0}$ and 
$\beta_{1}$ a was `r round(coef(fit)[1],2)` and `r 
round(coef(fit)[2],2)`, recall that the value used to generate the data 
was `r beta_0` and `r beta_1` for the relationship of abundance with depth. The 
estimate of $\beta_{0}$ and $\beta_{1}$ for the function relating 
capture probability to depth was `r round(coef(fit)[3],2)` and `r 
round(coef(fit)[4],2)`, recall the that value used was `r gamma_0` and `r 
gamma_1`. 


Ignoring uncertainties for the moment, we can estimate the total 
abundance given depth at each location in the study area as $N = \sum_{i=1}^{I} 
exp(`r round(coef(fit)[1],2)` \cdot `r round(coef(fit)[2],2)` \cdot 
depth_{i})$, where $i$ indexes each grid in the study area. 


The image below illustrates the true abundances for a simulated study area and 
the estimated abundances. 

```{r,echo=FALSE}
sa$pred<-predict(fit,type="state", 
    newdata=data.frame(depth=sa$depth))$Predicted
par(mfrow=c(1,2),oma=c(1,4,1,1))

## BREAKS FOR PLOTTING
brks<- seq(0,max(sa$pred,sa$abundance)+10,10)
col<- heat.colors(n=length(brks)-1)
z<- dcast(sa,x~y,value.var="abundance")
x<-z[,1]
y<-as.numeric(names(z[-1]))
z<- as.matrix(z[,-1])
image(x,y,z,
    xlim=c(0,7),
    xlab="X",
    ylab="Y",
    ylim=c(0,9),
    breaks=brks,
    col=col,
    asp=1,
    las=1,
    main="True")
# SHOW SAMPLING POINTS
points(y~x,sampleSites, pch=3)    
    
z<- dcast(sa,x~y,
    value.var="pred")
x<-z[,1]
y<-as.numeric(names(z[-1]))
z<- as.matrix(z[,-1])
image.plot(x,y,z,
    xlim=c(0,7),
    xlab="X",
    ylab="Y",
    ylim=c(0,9),
    breaks=brks,
    col=col,
    asp=1,
    las=1,
    main="Predicted")

```

Using the predictions the estimate of abundance in the study area is `r 
round(sum(sa$pred),0)` and the true abundance was `r 
sum(sa$abundance)`. There are design aspects where fine tuning can 
occur, such as modifying the number of replicates to take or how many 
sites to sample and where to put them. 
