---
title: ""
---

<!--
library(knitr)
rmarkdown::render_site("hw-03.Rmd")# build website

# rmarkdown::render_site()# build webpage
# COPY FILES TO DOCS FOR GITHUB.IO
system(paste("xcopy", 
    '"C:/Users/mcolvin/Documents/Teaching/WFA8433-Natural-Resource-Decision-Making/Course-Materials/_site"', 
    '"C:/Users/mcolvin/Documents/Teaching/WFA8433-Natural-Resource-Decision-Making/Docs"',
    "/E /C /H /R /K /O /Y")) 
  q(save="no") 
-->

```{r,echo=FALSE}
## create data for analysis
set.seed(8433)
tmp<-data.frame(elevation=runif(167,10,300),
    group=sample(c("g1","g2","g3"),167,replace=TRUE))
betas<- c(1,0.002,1,-0.5,0.0003,0.0006)
mm<- model.matrix(as.formula("~elevation+group+elevation:group"),
    tmp)
tmp$density<- exp(rnorm(167,mm %*% betas,0.2))
fit<- lm(density~elevation+group+elevation:group,tmp)
write.csv(tmp,"elevation-density.csv") 
```



```{r echo=FALSE, out.width="100%"}
include_graphics("media/banner-09.jpg")
```

# Homework 3: Predicting outcomes

The objectives of this homework assignment are to 
1. increase your skills in making predictions from analysis, 
2. incorporating uncertainty in predictions, 
3. using discretization, and
3. programming in R
3. parameterizing decision models


## Preliminaries

* Assignment due: Due by March 23rd by 5 pm 
* Submit R script and your netica file to: https://dropitto.me/WFA8433.
The upload password is 'critter' <u>Be sure to name the file as
follows "lastname-firstIntial-homework-03.R"</u> and
"lastname-firstIntial-homework-03.neta"</u>
* For help and assistance.
    * If you have problem with the questions you can work with classmates.
    * If you have any questions or issues about coding feel free to email me.  Please attach your R script so I can diagnose any issues.
    * If you have a problem and no one else can help, and if you can find them, maybe you can hire...the A-Team.

# Part I

```{r,echo=FALSE}
betas<- c(90,
    0.3,
    -2.3,
    1.3,
    0.4)

basal<- data.frame(
    temperature=round(runif(400, 60,90),0),
    habitat= sample(c("h1","h2","h3"),400,replace=TRUE),
    elevation=round(runif(400,0,200)),0)
dm<- model.matrix(as.formula("~temperature+habitat+elevation"),basal)
basal$basal<- round(dm %*% betas + rnorm(400,0,30),1)
write.csv(basal, "basal.csv",row.names = FALSE)
```

## Exercise 1 Predicting normal outcomes

1. Download the dataset for this exercise [here](basal.csv)
2. Read in the data and assign it to an object `basal`
3. Plot the relationship between the 3 covariates and the response variable
4. Fit a linear model relating `temperature`, `habitat`, and `elevation` to
`basal area` and name the resulting object `fit`
5. Use the expand.grid function to make a prediction dataset named `pred` for range of 
`temperature`, `habitat`, and `elevation` in the dataset.
6. Use the predict function to predict basal area for the new data `pred` and
add those predictions as a field in to `pred`
7. Add 95% prediction intervals and add them as a fields named `lci` and
`uci` for the lower and upper 95% prediction interval and add them to `pred`
8. Make 2 plots oriented as a single figure with 2 stacked plots (i.e., 2 rows, 1 column)
for the following:
    1. the predicted outcome (solid line) and 95% prediction intervals (dotted lines) for 
    `temperature` for a single value of `habitat` and `elevation`. 
    The values are your choice.  
    <!-- this one is terrible, need to revise
    2.the predicted outcome (solid line) and 95% prediction intervals (dotted lines) for 
    `habitat` values for a single value of `temperature` and `elevation`. 
    The values are your choice.
    --> 
    3. the predicted outcome (solid line) and 95% prediction intervals (dotted lines) for 
    `elevation` for a single value of `habitat` and `temperature`. 
    The values are your choice.
9. Make a new prediction dataset named `randdata` where `temperature`, `habitat`, and `elevation` are
100,000 values randomly drawn from a uniform distribution (`runif()`) for each input. 
Random values should be constrained to be between the minimim and 
maximum values for `temperature`, `habitat`, and `elevation` in the dataset. 
10. Use the `model.matrix()` function to create the design matrix named `dm` for the model fit
in step 4. 
11. Predict the basal area using `dm` and the $\beta$s extracted from `fit` and add
the predictions to `randdata` as a field named `dm`. 
12. Use to `predict()` function to predict basal area and add
the predictions to `randdata` as a field named `pred`.
13. Construct a scatter plot of `dm` and `pred` from the `randdata` dataset and 
add a 1:1 line. 


## Exercise 2 Discretizing outcomes


```{r,echo=FALSE}
betas<- c(90,
    0.3,
    -2.3,
    1.3)

insects<- data.frame(
    elevation=round(runif(36, 60,90),0),
    habitat= sample(c("h1","h2","h3"),36,replace=TRUE))
dm<- model.matrix(as.formula("~elevation+habitat"),insects)
set.seed(565)
insects$count<- rpois(36,dm %*% betas)
write.csv(insects, "insects.csv",row.names = FALSE)
```
1. Download the dataset for this exercise [here](insects.csv)
2. Read in the data and assign it to an object `insects`
3. Plot the relationship between the 2 covariates and the response variable
4. Fit a generalized linear model relating `habitat`, and `elevation` to
`count` and name the resulting object `fit`. Count is Poisson distributed.
5. Estimate the expected insect count for an elevation of 100 in habitat 2.
6. Generate 100 random outcomes for an elevation of 100 in habitat 2 given 
the $\beta$s estimated in `fit`. 
7. Discretize the 100 random outcomes into bins starting at 0 and ending at 
200 by increments of 25.  (See supplemental information below)
8. Calculate the probability for each bin in step 7.
What are the probabilities for the outcomes 0-25, 100-125, and 125-150. 
(See supplemental information below)
9. Repeat steps 6 to 8 for values 250, 500, 750, 1000, 1500, 2000, 3000, 5000,
10000, and 100000. (HINT a `for()` loop will help)
10. Make a plot with the number of stochastic replicates on the x-axis
and probability of the outcome 0-25 on the y-axis.
11. Repeat step 10 for the remaining outcomes 100-125, and 125-150.
12. How many stochastic replicates do you think you need to accurately quanity
they uncertainty in your discretized outcomes?


# Part II
```{r,echo=FALSE}
# Juvenile production
set.seed(23532)
juv_prod<- data.frame(
    out_loc= sample(c("MF Paddy's", "Crabtree Creek", "Thomas Creek"),130,replace=TRUE),
    survivors= round(runif(130,1,300),0))
X<- model.matrix(as.formula("~out_loc+survivors"),juv_prod)
betas<- c(0,0.25,0.1,0.015)

juv_prod$prod<-  rpois(130, exp(X %*% betas))
write.csv(juv_prod,"juvenile-production.csv",row.names=FALSE)


# Translocation survival
set.seed(23532)
tran_surv<- data.frame(
    handle_time= round(runif(130,1,10),0),
    transport_time= round(runif(130,10,100),0),
    n_fish= round(runif(130,1,300),0))
X<- model.matrix(as.formula("~handle_time+transport_time"),
    tran_surv)
betas<- c(0.75,-0.1,-0.01)
tran_surv$survivors<- rbinom(130,tran_surv$n_fish,plogis(X %*% betas))
write.csv(tran_surv,"transport-survivals.csv",row.names=FALSE)

```

```{r,echo=FALSE}
juv_prod<- read.csv("juvenile-production.csv")
tran_surv<- read.csv("transport-survivals.csv")



n<-500000
outcomes<- data.frame(
    out_loc= sample(c("MF Paddy's", "Crabtree Creek", "Thomas Creek"),n,replace=TRUE),
    n_fish= round(runif(n,1,300),0),
    survivors= round(runif(n,1,250),0),
    handle_time= round(runif(n,1,10),0),
    transport_time= round(runif(n,10,100),0))
X<- model.matrix(as.formula("~out_loc+n_fish"),outcomes)
fit<- glm(prod~out_loc+survivors,juv_prod,family="poisson")
outcomes$juv_prod<-  rpois(n, predict(fit,outcomes,type="response"))

fit<- glm(cbind(tran_surv$survivors, tran_surv$n_fish-tran_surv$survivor)~
    handle_time+transport_time, tran_surv,family="binomial")
outcomes$survival<- predict(fit,outcomes,type="response")
outcomes$survivors<- rbinom(n,outcomes$n_fish,outcomes$survival)


outcomes$n_fish_b<- binning_cont(outcomes$n_fish,0,300,50)
outcomes$juv_prod_b<- binning_cont(outcomes$juv_prod,0,175,25)
outcomes$handle_time_b<- binning_cont(outcomes$handle_time,0,10,2)
outcomes$transport_time_b<- binning_cont(outcomes$transport_time,10,100,10)
outcomes$transport_time_b<- binning_cont(outcomes$transport_time,10,100,10)
outcomes$survival_b<- binning_cont(outcomes$survival,0,1,0.10)
outcomes$survivors_b<- binning_cont(outcomes$survivors,0,250,50)

library(reshape2)

cpt_a<- dcast(outcomes, out_loc+survivors_b~juv_prod_b,
    value.var="survivors_b",length,drop=FALSE)
cpt_b<- dcast(outcomes, n_fish_b+survival_b~survivors_b,
    value.var="survival_b",length)
cpt_c<- dcast(outcomes, transport_time_b+handle_time_b~survival_b,
    value.var="survival_b",length)
write.csv(cpt_a, "cpt_a.csv",row.names=FALSE)
write.csv(cpt_b, "cpt_b.csv",row.names=FALSE)
write.csv(cpt_c, "cpt_c.csv",row.names=FALSE)
```


The part will work you through the skills you need to take analyses and use them 
in decision models. Below is a simple decision model that evaluates where to 
outplant spring Chinook Salmon in the Middle Fork Willamette River. Specifically 
we will be completing the inputs for nodes A, B, and C below. The rest of the 
nodes are already completed. So once Nodes A, B, and C are parameterized all 
that is left is to compile the model and you can evaluate outplant locations
objectively accounting for uncertainty! Pretty awesome right?

```{r echo=FALSE, out.width="80%",fig.align="center"}
include_graphics("media/hw-03-bdn.png")
```
The decision model illustrated above is [here](HW-03-Part-II.neta) as a
`\*.neta` file. 

The model evaluates 3 outplant locations given cost and juvenile 
production. Cost and juvenile production are scaled range from 0 to 1 
and weights of 0.5 given cost and juvenile production. On this scale 
high production approaches 1 and high cost approaches 0. Managers are 
trying to maximize bang for the buck, so a marginal gain is calculated 
as the utility as $U = \frac{Juvenile Production}{Cost}$. Aspects of the 
translocation program influence production, in particular there is 
mortality associated with moving fish and production varies among 
outplant locations. The exercises below step through how we use data to 
parameterize these models, building on the skills you used in Part I. 

Files needed for this exercise

* [Decision network](HW-03-Part-II.neta)  
* [Juvenile production](juvenile-production.csv)  
* [Translocation survival](transport-survivals.csv)  

These exercises might seem a bit arduous but they will push you to put the 
elements together to go from predicting outcomes while accounting uncertainty
and incorporating those outcomes in a decision model. The 4 main exercises
work through generating outcomes, discretizing outcomes, summarizing outcomes,
and parameterizing a decision model.

1. Predicting and discretizing Pr(Survival) and translocation
survivors (Nodes B and C in the figure above).
Data for this exercise is
[here](transport-survivals.csv). We need to start here because the other
nodes depend on this one. 
    1. Read in the `\*.csv` for this exercise and assign it as an object 
    `tran_surv` 
    2. Fit a GLM assuming the data is binomially distributed and an 
    additive function of `handle_time` and `transport_time`. HINT recall the 
    use of `cbind()` to put successes and failures together for a binomial 
    in the `glm()` function. 
    3. Create a new data.frame called `outcomes` with 500,000 stochastic 
    realizations using the `runif()` function for values of `handle_time`, 
    `transport_time`, and `n_fish` that vary from 1 to 10, 10 to 100, and 0 
    to 300 respectively, fractional values are fine the time components of 
    this simulation, fish should be whole numbers. (HINT for `n_fish` 
    field using the `runif()` 
    function and then round to the nearest whole number works very 
    efficiently for this purpose `round(4.56,digits=0)`). 
    4. Using the model fitted in 1.2 create a field in `outcomes` named 
    `survival` and predict the survival for each combination of 
    `handle_time` and `transport_time` in `outcomes`. 
    5. Discretize the values of `handle_time` `transport_time`, and 
    `survival` in `outcomes`, 
    6. Using the predicted survivals stochastically generate number of 
    survivors given `survival` and `n_fish` and name the field `survivors`
    using the `rbinom()` function. 
2. Juvenile production (Node A in the figure above). Data for this 
exercise is [here](juvenile-production.csv). 
    1. Read in the `\*.csv` for this exercise and assign 
    it as an object `juv_prod`
    2. Fit a GLM assuming that `prod` is Poisson distributed and 
    an additive function of `out_loc` and `survivors`.  
    4. Add a field named `out_loc` to `outcomes` that is 500,000 stochastic realizations 
    from the levels of outplant locations (HINT: `sample()`) 
    5. Using the model fitted in 2.2 create a field in `outcomes` named 
    `juv_prod` and generate 500,000 stochastic replicates from a Poisson 
    distribution given the values of `out_loc` and `survivors` (recall survivors 
    was created in 1.6 above) in `outcomes`. Note there is 1 stochastic 
    realization per combination of `out_loc` and `n_fish` in `outcomes`. 
3. Discretizing values for use in decision models
    1. Discretize the values of `out_loc`, `handle_time`,
    `transport_tim`,`n_fish` `survival`,`survivors`, and `juv_prod` in 
    `outcomes`, and name and bin as follows (*NOTE: Netica bins 
    continuous values as 0 to 50 and 50 to 100, which means values including 
    0 and less than 50 and values greater than or equal to 50 and less than 
    100. To deal with this we need to add an argument to the `cut()` 
    function, specifically `include.lowest=TRUE`. Be sure to add that 
    argument to discretize the values by the bins below*):
        1. `out_loc`: these values are already discrete, nothing to do with this one. 
        2. `handle_time_b`: 0 to 2, 2 to 4, 4 to 6, 
        6 to 8, and 8 to 10.
        3. `transport_time_b`: 10 to 20, 20 to 30, 30 to 40, 
        40 to 50, 50 to 60, 60 to 70, 80 to 90, 90 to 100.
        4. `n_fish_b`: 0 to 50, 50 to 100, 100 to 150, 150 to 200, 
        200 to 250, and 250 to 300. 
        5. `survival_b`: 0.2 to 0.3,0.3 to 0.4, 0.4 to 0.5, 
        0.5 to 0.6, and 0.6 to 0.7.
        6. `survivors_b`: 0 to 50, 50 to 100, 100 to 150, 150 to 200, 
        and 200 to 250.   
        7. `juv_prod_b`: 0 to 25, 25 to 50, 50 to 75, 75 to 100, 100 to 125,
        125 to 150, 150 to 175.
    2. Using the discretized values, calculate the frequency of each outcome.
    (i.e., `cut()`), use the discretized 
    values to calculate the frequency of each discrete outcome using the 
    steps below
        1. If you don't already have it make sure that the `reshape2` library 
        is loaded. Use `install.packages("reshape2")` if you do not have it 
        already and `load(reshape2)` to load the library. This will load the
        function `dcast()` which is useful for making contingency tables.
        2. Now you can run this bit of code to calculate the frequencies for 
        more complex data. Specifically we need to develop a contingency table 
        for the combinations of `n_fish_b`, `out_loc` and
        the discretized vector of outcomes `juv_prd_b` to make a table of
        frequencies of each outcome. `cpt_c<- 
        dcast(outcomes, transport_time_b + handling_time_b ~ survival_b, 
        value.var="survival", fun.aggregate=length, drop=FALSE)`. 
        We need to include 
        the `drop=FALSE` argument
        make sure we do not drop any of the bin levels. The code summarizes 
        values using the `length` function to tally up how many values were in 
        that combination. 
        3. Save `cpt_c` as a `cpt_c.csv` so you can get access to the counts so we 
        can add them to the CPT for node C in the decision model.
        4. Now we can do the same thing for nodes B and C, specifically:
            1.`cpt_b<- dcast(outcomes,survival_b+n_fish_b ~ survivors_b,
            value.var="survivors", fun.aggregate=length, drop=FALSE)` 
            2.`cpt_a<- dcast(outcomes,out_loc+survivors_b ~ juv_prod_b,
            value.var="juv_prod", fun.aggregate=length, drop=FALSE)` 
        3. The code above should get you 90% there to get the frequencies for each outcome. 
        5. Now we could convert those frequencies to relative frequencies, but 
        we do not need to do so, because Netica can use counts and calculate the
        probabilities for us. All we need to do is 
        copy the matrix of counts in the `cpt_a.csv`, `cpt_b.csv`, and 
        `cpt_c.csv`,then highlight the 
        entire table for the corresponding node (CTL + A or highlight with 
        your mouse) in the decision model. See a quick YouTube video illustrating
        this [here](https://youtu.be/c7OldCwM440).
        5. Now we paste the frequencies into the tables for nodes A, B, and C.
        *Be sure the order of your node rows match your `\*.csv` order.* 
4. Compiling the decision model
    1. Once the remaining nodes of the decision network have been filled
    in for you and all you need to do is compile the network.
    Go to Network ->  Compile.    
```{r echo=FALSE, out.width="80%",fig.align="center"}
include_graphics("media/netica-compile.png")
```
    2. Once the network is compiled, values in the decision node will 
    become visible in the blue box.
    3. In your R script report the locations, values, and the optimal 
    decision (i.e., location with the highest value).
    



# Supplemental functions and use

You may find the `cut()` function useful for this homework.
The cut function cuts up data into bins. The usage is

`cut(x=valuesToCut, 
    breaks=valuesToCutBy,
    labels=labelsForBins)`
    
One trick to using cut is to make sure the number of breaks is 1 more than
the number of labels. Ok, here is an example. We will use the `runif()` function
to generate 1000 values between 0.2 and 1. We want to bin those values into 
bins of 0-0.1, 0.1-0.2, ... 0.9-1. And then we want to tally up the frequency of 
each outcome. I can tell you that the frequency of outcomes falling in the 
0-0.1 and 0.1-0.2 bins should be 0!

```{r}
x<- runif(1000,0.2,1)
brks<- seq(from=0,to=1,by=0.1)
myLabels<-c("0-0.1","0.1-0.2",
    "0.2-0.3","0.3-0.4",
    "0.4-0.5","0.5-0.6",
    "0.6-0.7","0.7-0.8",
    "0.8-0.9","0.9-1")

binnedValues<- cut(x=x,
    breaks=brks,
    labels=myLabels)
table(binnedValues)# get the frequency
```
Another way to do the above using `paste()` function.

```{r}
x<- runif(1000,0.2,1)
brks<- seq(from=0,to=1,by=0.1)
myLabels<-paste(brks[-length(brks)],brks[-1],sep="-")
binnedValues<- cut(x=x,
    breaks=brks,
    labels=myLabels)
table(binnedValues)# get the frequency
```

The table function is nice too because it fills in 0s for 
values that are not present but the label is.
We can calculate the probability of each outcome as

```{r}
myFreqs<-table(binnedValues)
prop.table(myFreqs)# get probability
```



 
