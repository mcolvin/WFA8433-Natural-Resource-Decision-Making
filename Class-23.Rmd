
---
title: ""
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true   
    collapsed: false
---

 <!--
rmarkdown::render_site("Class-23.Rmd")# build website
library(knitr)
rmarkdown::render_site()# build website

source("_build.R")
build("Class-23",bld="PAGE",docs=TRUE) # bld = PAGE,ENTIRE,SCRIPT

# PURL THIS SHIT & MOVE FILES TO DOCS
build("Class-23",bld="SCRIPT",docs=TRUE) # bld = PAGE,ENTIRE,SCRIPT

source("_build.R");build("Class-23",bld="PAGE",docs=TRUE)# bld = PAGE,ENTIRE,SCRIPT
-->


```{r echo=FALSE, out.width="100%"}
library(knitr)
include_graphics("media/banner-06.jpg")
```


# Class 23: Adaptive management case studies {-}

# Class preliminaries

* Supplemental background reading:
    * Marescot, L., G. Chapron, I. Chad√®s, P. L. Fackler, C. Duchamp, E. 
    Marboutin, and O. Gimenez. 2013. Complex decisions made simple: A primer 
    on stochastic dynamic programming. Methods in Ecology and Evolution 
    4:872-884. [PDF](pdfs/M292.pdf )
    * Conroy, M. J., M. W. Miller, and J. E. Hines. 2002. Identification 
    and Synthetic Modeling of Factors Affecting American Black Duck 
    Populations. Wildlife Monographs:1-64. [PDF](pdfs/C267.pdf ) 
    * Anderson, D. R. 1975. Optimal Exploitation Strategies for an Animal 
    Population in a Markovian Environment: A Theory and an Example. Ecology 
    56:1281-1297.[PDF](pdfs/A192.pdf )  
* Reading(s) for next time:
    * Conroy and Peterson Chapter 6
    * Conroy and Peterson Chapter 9
* Class project presentation during final exam period-April 28th at 3pm.
* Link to class recording  [YouTube](https://youtu.be/j4SMWCkOevs)
* Today's R script [Class-23.R](scripts/Class-23.R)


```{r}
# install.packages("MDPtoolbox")
# install.packages("msm")
# install.packages("fitdistrplus")
# install.packages("Hmisc")
```

## Class overview & objectives 

The objectives of this class are to:

1. Further understand of adaptive resource management
2. Provide background on the use of elicitation to parameterized decision models
  




# Management and monitoring.

## Overview


### What does monitoring provide in ARM

In ARM, monitoring provides: 

1. An estimate of the current state of the system before a decision is 
made (where we are). Remember that decisions are state dependent. 
2. After a management alternative is implemented, monitoring provides 
information on what changes, if any, occurred to the system (where
did we end up), and
3. Most importantly, the this also should provide information on the 
system dynamics that should reduce uncertainty and improve future 
decision making and returns (what did we learn). 


### Monitoring and Adaptive Resource Management

It should be clear that monitoring and models are tightly linked in 
adaptive resource management in a formal process. The model weights are 
updated by comparing predictions to observed outcomes. For the process 
to work effectively and efficiently predicted and measured responses 
should be on the same unit scale and in the same units. For example, if 
models estimate population size then, monitoring should estimate the 
number of animals in the population rather than estimating some index of 
population size, such as relative abundance. 


|        Model prediction        	|           Monitoring variable          	|
|:------------------------------:	|:--------------------------------------:	|
|         Population size        	|                Abundance               	|
|        Species richness        	|            Number of species           	|
| Species occupancy/distribution 	| Number or proportion of areas occupied 	|
|           Area burned          	|          Amount of area burned         	|


Decision makers also should take great care to avoid systematically 
biased measures, such as raw counts that are unadjusted for incomplete 
detection (e.g., population indices, catch-effort indices). Biased 
measures can provide misleading information that can lead to bad 
management decisions. Misleading information can have negative 
value. 


### What to monitor?

*Monitoring other decision model components should include*

* key drivers of outcomes
* values needed to estimate expected outcome after decision
* values useful for explaining unanticipated outcomes

Choosing what to monitoring as part of an adaptive resource management 
plan will vary from plan to plan and depends largely on the resources 
available to the decision maker (e.g., personnel, equipment, funds) and 
the sources and levels of uncertainty in the decision model. Of course, 
the most important components to monitor are the valued outcomes that 
are used to calculate the utility for example:

* population size, 
* distribution, or
* harvest. 

These must be monitored. Monitoring other components of the 
decision model should focus on the key drivers of the of the outcomes 
that are identified during sensitivity analysis like

* habitat 
* availability, 
* annual temperature, and 
* precipitation.

These data are needed to plug into the model to estimate the expected 
outcome after the decision and also can be useful for explaining 
unanticipated outcomes. 

 

### Single and double loop learning

*The ARM process is flexible and decision makers can use loop learning*

* The single loop provides the iterations to make decisions
* The double loop is the time to reassess and reevaluate
* Caveat emptor: you need to be sure to stay in the single loop 
long enough to learn!

We have described ARM as a special case of structured decision 
making that involves dynamic decision making, multiple models 
representing alternative hypotheses of system dynamics, and monitoring. 
Monitoring is used in an iterative process that provides feedback to 
reduce uncertainty about the dynamics of the system being modeled. This 
iterative process has been defined as single loop learning. 

Single loop learning in the context of ARM begins after the initial 
structured decision making processes is completed. That is, objectives 
and decision alternatives have been identified and the alternative 
decision models built. Learning within the single loop occurs with 
respect to the given (fixed) set of objectives, alternatives, and models 
and learning occurs relatively frequently (e.g., annually). 

Through time decision makers may find that their current set of models 
is inadequate or that management objectives or decision alternatives are 
insufficient and need to be changed. 

In double loop learning, the management objectives, decision 
alternatives, and models are reassessed and potentially revised to 
reflect changes in scientific knowledge and management objectives and 
alternatives. Learning in the outer loop occurs at a much slower rate 
and with slower frequency (e.g., every 10 years) compared to single loop 
learning (e.g., every year). 

There is no general rule when to initiate the reassessment as 
it will vary from program to program and largely depends on the decision 
makers, stakeholders, and technical experts. For example, the US Fish 
and Wildlife Service conducts endangered species status assessments 
approximately every 5 years, so the reassessment of objectives, 
alternatives and models (i.e., the outer loop) may coincide with 
planned. However, decision-makers should try to minimize the frequency 
of the reassessments to allow for sufficient amount of time to 
accumulate information. 

```{r echo=FALSE, out.width="100%"}
library(knitr)
include_graphics("media/class-22/loop-learning.png")
```


### Generalization of Adaptive Resource Management

ARM is fairly flexible and can be modified to fit the decision 
situation. 

* Decisions, monitoring, and feedback (i.e., updating weights) can occur 
at different intervals. 
    * the decision to use controlled burns to manage vegetative structure 
    may be revisited every 3 to 5 years but monitoring the vegetative 
    structure can occur annually. 
* Some decisions also may not be revisited as frequently 
as others due to slow system response or as a result of legislative 
mandates. 
    * For example, the US Federal Energy Regulatory Commission licenses 
    private, municipal, and state hydroelectric projects for periods that 
    range from 20- 50 years. The licenses include conditions for which 
    licensees must comply, such as dam operation restrictions designed to 
    minimize impacts to fish and wildlife. Thus, dam operation decisions are 
    revisited over long time intervals. 
* In instances of long periods between decision...
    * It may be more efficient to incorporate spatial feedback where the 
    decision is made on one project (e.g., a hydropower dam), monitoring is 
    conducted and used to resolve uncertainty about system dynamics, and the 
    updated beliefs used to identify the optimal decision at another 
    location (e.g., another hydropower dam). 
    * Sequential dynamic decision making in space, 
    however, requires that the same set of decision alternatives are 
    available to decision makers at each managed system. In addition, the 
    systems should be sufficiently similar so that the same model set can be 
    used to predict the outcomes of management actions. 

    
### Basic Forms of Adaptive Resource Management

    
    
**There are two basic forms of adaptive resource management:**

1. passive
2. active

The ARM process is the same for both forms. Alternative models 
are used to identify the optimal decision and monitoring data are used 
to improve the understanding of system dynamics. 

**Passive ARM**  
In passive ARM, decisions are chosen as if the current uncertainty about 
the system dynamics will not change. That is, the decision is chosen 
based long term gain in management objectives (utility) assuming the 
model weights will not change. Information gained through monitoring for 
passive ARM is incorporated, but not in a planned way. 

**Active ARM**
Active ARM takes into account how reducing uncertainty can 
affect the long term gain. For example, a particular decision at a point 
in time may resolve key uncertainties quicker or more efficiently than 
the other decisions and resolution of the uncertainty will result in 
better decision making and greater long term gain in management 
objectives. Active ARM generally does not involve experimentation or 
probing of the system. Probing can, in fact, reduce the long term gain. 
Probing is only valued when uncertainty is very high and management loss 
is expected to be high if the uncertainty is not resolved. Taking into 
account learning to improve management is termed dual control. 


|                                   Passive ARM                                   	|                                                          Active ARM                                                          	|
|:-------------------------------------------------------------------------------:	|:----------------------------------------------------------------------------------------------------------------------------:	|
|                                Relatively simple                                	|                                                 Computationally more complex                                                 	|
| Choose decisions as if current uncertainty will not change                      	| Potential information returned by each decision is given value when evaluating each decision                                 	|
| Information gained through monitoring is incorporated, but not in a planned way 	| "Probing" is valued when: 1) uncertainty is very high 2) management loss is expected,to be high if uncertainty is,unresolved 	|


**Policies under the 2 approaches can differ because passive does not 
anticipate learning.**


### ARM and conflict resolution 


Adaptive resource management can be useful for resolving potential 
conflicts among stakeholders, provided the disagreement is about 
science. Especially if 
* stakeholders agree on objectives but 
* have different ideas about how the system works.

The differing ideas can be incorporated into the decision model as 
alternative models. Monitoring then can be used to resolve the 
uncertainty. 

 

However, not all apparent disagreements on science are truly about the 
science or "your model versus my model." Sometimes, scientific 
disagreement is used to mask disagreements over stakeholder objectives. 


```{r echo=FALSE, out.width="50%"}
library(knitr)
include_graphics("media/class-23/conflict.png")
```

In these instances, ARM is not the appropriate tool, but the ARM process 
can be used to reveal these potential problems. 


### Myths about ARM

**There are some common misconceptions about ARM; chief among these is:**

* It's research, no it is management
    * Adaptive Resource Management is first and foremost, 
    management. All decisions are made to maximize management objectives. 
    Learning occurs as a byproduct of management and it improves decision 
    making thereby increasing the long term gain to managers.
* It's too risky
    * All natural resource management decision making involves risk in one 
    form or another (e.g., over harvesting a population vs. aggravating user 
    groups) and uncertainty increases risk. ARM reduces uncertainty and risk. 
* It distracts from management goals
    * ARM also does not distract from management goals because learning is 
    focused at improving management. 
* It's too complicated and technical
    * Most agencies already incorporate most, if not all, of the components of ARM!
        * Decisions
        * Objectives
        * Models (implicit rather than explicit)
        * Monitoring
    * What is needed: linking decisions to objectives with explicit model and
    targeted monitoring

    
## Case study: Adaptive harvest management for American black ducks

For more background see  

* Conroy and Peterson Chapter 9
* Anderson 1975
* Conroy, Nichols, and Hines


### Black ducks have issues

* International resource
* Populations have declined from historical levels but may be recovering
* Controversy over causes of the decline and appropriate management
    * Harvest
    * Mallards

### Reasons for Black Duck AHM

* Development a continental strategy for managing black duck harvest
* Deal with (not resolve) issues of uncertainty in black duck dynamics
    * Mallards
    * Harvest
* Use predictive models and monitoring to:
    * Make optimal, or at least 'good', harvest decisions
    * Learn

### Stakeholders and governance

* Stakeholders
    1. Black Duck Adaptive Harvest Management Working Group 
    1. Black Duck International Harvest Strategy Committee
    1. Canadian Wildlife Service
    1. U.S. Fish and Wildlife
    1. US Geological Survey
* Governance Structure: Consensus decisions on 
    * Objectives
    * Tech issues



### Fundamental Objectives

**Maximum sustainable harvest**

* Constrained by
    * Population goals-penalty to harvest in N<N*
        * Penalty applied to harvest if projected to fall below 500,000 ducks in spring surveys
    * Parity goals-penalty to harvest if p>p* (proportion of harvest in 1 country)
        * Assure that neither country gets disproportionate share of harvest
        * Penalty if US-Canada proportions of harvest outside range of 0.4 to 0.6
        * Penalty applied to harvest in country with harvest > 0.6


### Decision situation

* Decision context
    * Manage black ducks such as to maximize their harvest in the US and Canada
* Spatial dimensions
    * Eastern half of North America (below)
* Planning horizon
    * Decisions are made annually 
    * evaluate dynamics over 100 year time period
* Decision alternative
    * Season length 
    * bag limits

```{r echo=FALSE, out.width="75%"}
knitr::include_graphics("media/class-23/ducks-01.png")
```
 

### Structural uncertainty

**Alternative models were used to represent hypotheses of reproduction 
and survival processes influencing population dynamics.** 

1. Reproduction
    1. Competition effect from mallards
    2. No competition effect
2. Survival
    1. Density dependence (compensatory)
    2. Density independence (additive)

### Model parameterization 
 
**A combination of statistical and calibrated predictive models were 
used to predict outcomes of varying harvest rates.**

1. Fit of historical fall age ratio estimates
    * Generalized linear models 
    * Black duck abundance, mallard abundance, and habitat as predictors
2. Fit of historical non-harvest survival estimates 

   
### Identifying optimal policy


A process called stochastic dynamic programming was used to 
identify the optimal **state dependent** harvest policy illustrated 
below that maximizes harvest over a 100 year period. 

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("media/class-23/ducks-02-policies.png")
```


### Under the hood with stochastic dynamic programming


Example from Conroy and Peterson 2013 Passive ARM SDP example in Appendix E

```{r}
## Stochastic dynamic programming requires this library
library(MDPtoolbox)
```

```{r}
decisions<- c(0.1,0.2,0.3)
abundance<- c(5,10,15)
# With a decision specific transition matrices matrix
P <- array(0, c(3,3,3))
```

```{r}
#### weight of model 1
mod1.wt<- 0.5
```

```{r}
## decision 1 harvest = 0.1
Model1.1 <- matrix(c(0.2, 0.5, 0.3, 0.2,.3,0.5,0.1,0.3,0.6), 3, 3, byrow=TRUE)
Model2.1 <- matrix(c(0.3, 0.5, 0.2, 0.3,0.3,0.4,0.2,0.3,0.5), 3, 3, byrow=TRUE)
## model averaged transition matrix
P[,,1] <- Model1.1*mod1.wt + Model2.1*(1-mod1.wt)
```

```{r}
## decision 1 harvest = 0.2
Model1.2<- matrix(c(0.5,0.3,0.2,0.2,0.5,0.3,0.2,0.4,0.4), 3, 3, byrow=TRUE)
Model2.2<- matrix(c(0.7,0.3,0.0,0.4,0.5,0.1,0.5,0.4,0.1), 3, 3, byrow=TRUE)
P[,,2] <-  Model1.2*mod1.wt + Model2.2*(1-mod1.wt)
```

```{r}
## decision 1 harvest = 0.3
Model1.3<- matrix(c(0.7,0.3,0.0,0.6,0.3,0.1,0.3,0.5,0.2), 3, 3, byrow=TRUE)
Model2.3<- matrix(c(0.9,0.1,0.0,0.7,0.3,0.0,0.4,0.6,0.0), 3, 3, byrow=TRUE)
P[,,3] <-  Model1.3*mod1.wt + Model2.3*(1-mod1.wt)
```

Now if we make a reward matrix, the numbers harvested, we 
can use that to find the optimal policy. 

```{r}
##Reward matrix
R <- matrix(c(0.5,1.0,1.5,
    1,2,3,
    1.5,3.0,4.5), 
    nrow=3, 
    ncol=3, 
    byrow=TRUE)
```

Without going into the details, although they can be found in Appendix E 
of Conroy and Peterson, the transitions and reward matrix can be 
iterated over to identify the policy that maximizes the reward over the 
long term. This allows us to move beyond the maximizing the utility
in the next year to addressing sustainability. SDP is done in R using
the  `mdp_policy_iteration()` function from the `mdptoolbox` library.
Let's see how it works below.


```{r}
## heres the better way it automatically  iterates and stops when the policy is stable
out<-mdp_policy_iteration(P=P, R=R, discount=.99999)
```

The function returns the index of the optimal decision, so we can use
that index to build a policy table.

```{r}
policyTable<- data.frame(Abundance= abundance,
    HarvestRate=decisions[out$policy])
```

Now that was a case where the transition matrices were already put 
together, but how do we make them?  Let's put this in context. This
example is from Anderson's 1975 duck model. In that model he proposed
an additive and compensatory effect of harvest mortality on adult
and juvenile survival. Let's see how that works.

Here we will evaluate the optimal policy for duck abundances varying
from 1 to 15 million assuming there are 2 million ponds on the landscape.

The decision here is the harvest rate which 5 levels are evaluated varying
from 0 to 0.4.

The code below sets this up.

```{r}
N_t<- c(1:15)
H_t<- seq(0,0.4,by = 0.1)
combo<-expand.grid(
    N_t=N_t,
    P_t=2,
    H_t=H_t,
    rep=1:5000)
```

Now we start to produce the population dynamics. The equation below
predicts the number of young production to be added to the fall
population as a function of the number of ducs and ponds along 
with uncertainty.

```{r}
#  young production to be added to fall population (equation 2 (Anderson 1975))
combo$Y_t  = 1/((1/(12.48*combo$P_t^0.851))+
    (0.519/combo$N_t)) 
# add uncertainty
library(msm) 
set.seed(8483)#for reproducability
combo$Y_t <- rtnorm(length(combo$Y_t),
    combo$Y_t,
    combo$Y_t*0.3,
    lower = 1)
```

Now we can calulate the fall population as a survival rate
times the numbers of adults plus the number of young.

```{r}
# Fall population at time t (equation 5 (Anderson 1975))
combo$F_t	= (0.92*combo$N_t) + combo$Y_t 
```
Now we can set up how many ducks were harvested given the 
harvest rate and the number of ducks. We will need to keep track
of this number becuase it will be how we value different harvest 
rates.

```{r}
# Harvest at time t (need to keep track of this)
combo$harvest<-ifelse(combo$F_t < combo$H_t*combo$F_t,
    combo$F_t, 
    combo$H_t*combo$F_t)
```
Now we can see what the population size will be in the next year
given 2 hypotheses of how harvest mortality interacts with natural
mortality, in an additive or compensatory way.

The code below predicts survival assuming harvest mortality is 
additive and we can calculate what the population will be in the 
next year.

```{r}
# AMH: ADDITIVE MORTALITY
combo$survival_adult_amh<- (1-0.27*exp(2.08*combo$H_t))
combo$survival_young_amh<- (1-0.40*exp(0.67*combo$H_t))
# Pop size after spring migration
# AMH: ADDITIVE MORTALITY
combo$N.t1.amh<- combo$N_t*combo$survival_adult_amh + 
    combo$Y_t*combo$survival_young_amh
```
We can do the same thing for the compensatory model.

```{r}
# CMH: COMPENSATORY MORTALITY
combo$survival_adult_cmh<- ifelse(combo$H_t<0.25,
    0.57,
    (0.57-1.2*(combo$H_t-0.25)))
combo$survival_young_cmh<-  ifelse(combo$H_t<0.25,
    0.5,
    (0.5-1*(combo$H_t-0.25)))
# Pop size after spring migration
# CMH: COMPENSATORY MORTALITY
combo$N.t1.cmh<- combo$N_t*combo$survival_adult_cmh + 
    combo$Y_t*combo$survival_young_cmh
```

One of the quirks we have to deal with here is that new 
states can arise and that messes things up. Specifically, 
if we have 1 million ducks out there the future population 
might be less than 1 million ducks. The way we treat this is
to simply make that outcome 1 million. We can also have the case 
when the population in the next year may be more than 15 millions.
We simply round that outcome down to 15. And we also need to 
turn future abundance into a whole number, which the `floor()` function
nicely accomplishes for us. We will see why
this is important here in a sec.


```{r}
# keep new states from arising
combo$N.t1.amh  <- ifelse(combo$N.t1.amh>15,
    15,
    combo$N.t1.amh)
combo$N.t1.amh  <- floor(ifelse(combo$N.t1.amh<1,
    1,
    combo$N.t1.amh))
combo$N.t1.cmh  <- ifelse(combo$N.t1.cmh>15,
    15,
    combo$N.t1.cmh)
combo$N.t1.cmh  <- floor(ifelse(combo$N.t1.cmh<1,
    1,
    combo$N.t1.cmh))
```

Now that that nuisance is taken care of we can calculate the 
transition probabilities as a conditional probability matrix.
This will be a 15 by 15 matrix if we do it right and it will
have 4 matrices in the array. The table function will tally 
up the frequencies for us for each abundance state and harvest
level.
    
```{r}
## create a table of transition frequencies that will be turned into 
## state transition probabilities one for each decision alternative
TM_amh<- table(combo$N_t,combo$N.t1.amh,combo$H_t)
TM_cmh<- table(combo$N_t,combo$N.t1.cmh,combo$H_t)
```

We can use the `prop.table()` but we will use a special argument
`margin=c(1,3)` which calculates the conditional probability for
each row by each matrix.



```{r}
# These are now transition matrices one for each  
# harvest decision alternative
TM_amh<- prop.table(TM_amh,
    margin=c(1,3))
TM_cmh<- prop.table(TM_cmh,
    margin=c(1,3))
```

We now need to figure out the value, utility, or reward to 
maximize. In this case it is harvest and we simply need to 
figure out the probability for each harvest level given the 
harvest rate and intial population size. 
The `tapply()` function works well for this. 


```{r}
## calculate the average (expected) return for each population
## state / decision alternative combination
harvest<-tapply(X=combo$harvest,
    INDEX=list(combo$N_t,combo$H_t), 
    FUN=mean)
```

And just like before we can combine the transition matrices
into a single matrix by multiplying the values by the prior
and summing the elements but we need to do it by each 
harvest level.

Assuming complete uncertainty we can assign a prior probability
of 0.5 to the additive and 0.5 to the compensatory hypotheses.

```{r}
## Weight and add model specific population size estimates
prior_amh<- 0.5
prior_cmh<- (1-prior_amh)
TM<- TM_amh*prior_amh + prior_cmh*TM_cmh
```

Now we have transition matrices representing population dynamics
given the varying harvest rates. And we have matrix of rewards. 
Now we can use SDP to identify the optimal harvest policy for each
abundance state.

```{r}
### now find optimal state dependent harvest 
out<- mdp_policy_iteration(P=TM, 
    R=harvest, 
    discount=.999,
    policy0=rep(4,15),
    max_iter=100)
``` 

We can use the output to construct a policy table. This gives us
the optimal harvest policy for each abundance state.

```{r}
policyTable<- data.frame(Abundance= N_t,
    HarvestRate1=H_t[out$policy])
policyTable
```

Now, suppose we did some monitoring and was able to update
our prior probabilities for each hypotheses, we can simply
rerun the SDP and see the changes in optimal harvest policy.
In this case we there is more evidence for compensation then 
harvest rates should be higher after we learn this. Let's see
if that happens.

All we need to do is reweight the transition matrices to reflect
this new knowledge.

```{r}
## Weight and add model specific population size estimates
prior_amh<- 0.25
prior_cmh<- (1-prior_amh)
TM<- TM_amh*prior_amh + prior_cmh*TM_cmh
```
And use that updated transition matrix in the policy iteration.

```{r}
### now find optimal state dependent harvest 
out<- mdp_policy_iteration(P=TM, 
    R=harvest, 
    discount=.999,
    policy0=rep(4,15),
    max_iter=100)
```
Now let's add those new policies to the policy table and see if 
they adapted to the new information.


```{r}
policyTable$HarvestRate2<- H_t[out$policy]
policyTable
```
 
### Monitoring

1. Continent-wide surveys
2. Bird banding
3. Harvest reporting

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("media/class-23/ducks-03-monitoring-map.png")
```

### Case study conclusions

Current data and models sufficient to build a realistic AHM for black 
ducks. Objectives of black duck international harvest management can be 
addressed in an AHM context Monitoring programs sufficient for 
adaptation but rates of learning may be slow 


## ARM Summary

Adaptive resource management is a special case of structured decision 
making that is focused on 

1. reducing uncertainty, and 
2. improving decision making. 

ARM requires:

* Explicit prediction of the consequences of management actions using multiple models of system dynamics (i.e., hypotheses)
* Sequential dynamic decisions in time and/or space, and 
* Feedback in the form of monitoring data. 

Monitoring plays a crucial role is ARM. It provides:

* An estimate of the current state of the system,
* Information on system dynamics by comparing actual to predicted outcomes, and
* Integration of information direct and increases the value of decisions

   
<!-- #######################################################-->
<!-- #######################################################-->
<!-- #######################################################-->
<!-- #######################################################-->
<!-- #######################################################-->
<!-- #######################################################-->
<!-- #######################################################-->
